{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"api/","title":"API Reference","text":"<p>PyGrad - A neural network library with autograd automatic gradient calculation.</p> <p>@Author Adam Jamrozi\u0144ski</p> <p>Variable is the building block of the computation graph each overloaded or custom operator saves itself as a record on a tape stack</p> <p>The _tape_stack is bundled with the variable module by default. It contains all the information about which variable was needed to compute the other. The information how to compute the backward partial derivative at each step. Thanks to this approach, the list is being automatically sorted topologically (in reverse order)</p> <p>Each variable in the computation graph can be passed to the grad(...) function to compute its derivative</p> <p>The backward pass is computed via a topologically sorted stack of callbacks aka the backward pass is \"tape-based\". Tape class organizes and singles out Variable's responsibility of storing the computation graph (DAG). Additionally, it is a neat way we topologically sort the computation graph, since the computation is going to be performed in the same order we put it on the tape.</p>"},{"location":"api/#src.Variable","title":"Variable","text":"Source code in <code>src/autodiff/variable.py</code> <pre><code>class Variable:\n    __slots__ = (\"name\", \"value\")\n\n    def __init__(self, value: numeric | Iterable[numeric] = None, name: str = None):\n        self.name = name or free_name()\n        self.value = value if isinstance(value, ndarray) else array(value)\n\n    # TODO how to memoize everything by default?\n    @staticmethod\n    @lru_cache(None)\n    def constant(value: numeric, name: str = None):\n        \"\"\"micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph\"\"\"\n        return Variable(value=value, name=name)\n\n    @staticmethod\n    def convert_input_into_variable(\n        operator_impl,\n    ) -&gt; Callable[[Any, ...], Sequence[Variable]]:\n        @functools.wraps(operator_impl)\n        def operator_wrapped(self, *raw_inputs: numeric | Variable):\n            if not TYPE_SAFE:\n                return operator_impl(self, *raw_inputs)\n            converted_args: list[Variable] = []\n            for raw_other in raw_inputs:\n                converted: Variable\n                match raw_other:\n                    case ndarr if isinstance(ndarr, ndarray):\n                        # TODO check supported dimensions\n                        converted_args.append(Variable(raw_other))\n                    case raw_numeric if isinstance(raw_numeric, (int, float)):\n                        converted_args.append(Variable.constant(raw_numeric))\n                    case already_variable if isinstance(already_variable, Variable):\n                        converted_args.append(already_variable)\n                    case _:\n                        raise TypeError(\n                            f\"Unsupported type for Variable operation: {type(raw_other).__name__}\"\n                        )\n            return operator_impl(self, *converted_args)\n\n        return operator_wrapped\n\n    def __repr__(self):\n        return f\"{self.name}(value={self.value})\"\n\n    # operators have no explicit type checks for micro optimization as static typechecks should be enough\n    @convert_input_into_variable\n    def __add__(self, other: Variable | numeric):\n        return operator_add(self, other)\n\n    def __radd__(self, other: Variable | numeric):\n        return self + other\n\n    @convert_input_into_variable\n    def __neg__(self, other: Variable | numeric):\n        return operator_neg(self)\n\n    @convert_input_into_variable\n    def __sub__(self, other: Variable | numeric):\n        return operator_add(self, -other)\n\n    @convert_input_into_variable\n    def __rsub__(self, other: Variable | numeric):\n        return operator_add(-other, self)\n\n    @convert_input_into_variable\n    def __mul__(self, other: Variable | numeric):\n        if TYPE_SAFE:\n            if self.shape != other.shape:\n                other = self.broadcast_to(self.shape)\n        return operator_mul(self, other)\n\n    def __rmul__(self, other: Variable | numeric):\n        # elementwise multiplication forced commutative\n        return self * other\n\n    @convert_input_into_variable\n    def __matmul__(self, other: Variable | ndarray):\n        if TYPE_SAFE:\n            if other.ndim == 1:\n                other = other.reshape_as_matrix()\n        return operator_matmul(self, other)\n\n    @convert_input_into_variable\n    def __rmatmul__(self, other: Variable | ndarray):\n        return operator_matmul(other, self)\n\n    def flip(self) -&gt; Variable:\n        return operator_flip(self)\n\n    def pad(self, pad_width: Sequence[tuple[int, int]]) -&gt; Variable:\n        return operator_pad(self, pad_width=pad_width)\n\n    def sum(self, axis: tuple[int, ...] | int | None = None, keepdims = False) -&gt; Variable:\n        return operator_sum(self, axis=axis, keepdims=keepdims)\n\n    def broadcast_to(self, shape) -&gt; Variable:\n        return operator_broadcast_to(self, broadcast_shap=shape)\n\n    def reshape(self, shape) -&gt; Variable:\n        return operator_reshape(self, new_shape=shape)\n\n    @property\n    def shape(self):\n        return self.value.shape\n\n    @property\n    def dtype(self):\n        return self.value.dtype\n\n    @property\n    def T(self) -&gt; Variable:\n        return operator_transpose(self)\n\n    @property\n    def ndim(self):\n        return self.value.ndim\n\n    @property\n    def size(self):\n        return self.value.size\n\n    @staticmethod\n    def convolve(matrix: Variable, kernel: Variable):\n        \"\"\"Convolve matrix with kernel, both\"\"\"\n        if TYPE_SAFE:\n            assert matrix.value.ndim == kernel.value.ndim, (\n                \"convolution arguments dimensionality must match.\"\n            )\n            assert (\n                matrix.value.shape[0] &gt; kernel.value.shape[0]\n                and matrix.value.shape[1] &gt; kernel.value.shape[1]\n            ), (\n                \"Kernel size must be smaller than input size for convolution. Use elementwise multiplication instead.\"\n            )\n\n        return operator_convolution(matrix, kernel)\n\n    @staticmethod\n    def ReLU(variable: Variable):\n        return activation_ReLU(variable)\n\n    @staticmethod\n    def sigmoid(variable: Variable):\n        return activation_sigmoid(variable)\n\n    @staticmethod\n    def tanh(variable: Variable):\n        return activation_tanh(variable)\n\n    def backward(\n        self, directional_grad: Variable | None = None\n    ) -&gt; dict[Variable, Variable]:\n        \"\"\"Convenience method to compute gradients of this variable with respect to all other variables in the graph.\"\"\"\n        global _tape_stack\n        return grad(self, custom_entry_gradient=directional_grad, tape_records=_tape_stack)\n</code></pre>"},{"location":"api/#src.Variable.backward","title":"backward","text":"<pre><code>backward(directional_grad: Variable | None = None) -&gt; dict[Variable, Variable]\n</code></pre> <p>Convenience method to compute gradients of this variable with respect to all other variables in the graph.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def backward(\n    self, directional_grad: Variable | None = None\n) -&gt; dict[Variable, Variable]:\n    \"\"\"Convenience method to compute gradients of this variable with respect to all other variables in the graph.\"\"\"\n    global _tape_stack\n    return grad(self, custom_entry_gradient=directional_grad, tape_records=_tape_stack)\n</code></pre>"},{"location":"api/#src.Variable.constant","title":"constant  <code>cached</code> <code>staticmethod</code>","text":"<pre><code>constant(value: numeric, name: str = None)\n</code></pre> <p>micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>@staticmethod\n@lru_cache(None)\ndef constant(value: numeric, name: str = None):\n    \"\"\"micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph\"\"\"\n    return Variable(value=value, name=name)\n</code></pre>"},{"location":"api/#src.Variable.convolve","title":"convolve  <code>staticmethod</code>","text":"<pre><code>convolve(matrix: Variable, kernel: Variable)\n</code></pre> <p>Convolve matrix with kernel, both</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>@staticmethod\ndef convolve(matrix: Variable, kernel: Variable):\n    \"\"\"Convolve matrix with kernel, both\"\"\"\n    if TYPE_SAFE:\n        assert matrix.value.ndim == kernel.value.ndim, (\n            \"convolution arguments dimensionality must match.\"\n        )\n        assert (\n            matrix.value.shape[0] &gt; kernel.value.shape[0]\n            and matrix.value.shape[1] &gt; kernel.value.shape[1]\n        ), (\n            \"Kernel size must be smaller than input size for convolution. Use elementwise multiplication instead.\"\n        )\n\n    return operator_convolution(matrix, kernel)\n</code></pre>"},{"location":"api/#src.autodiff.variable.Variable","title":"Variable","text":"Source code in <code>src/autodiff/variable.py</code> <pre><code>class Variable:\n    __slots__ = (\"name\", \"value\")\n\n    def __init__(self, value: numeric | Iterable[numeric] = None, name: str = None):\n        self.name = name or free_name()\n        self.value = value if isinstance(value, ndarray) else array(value)\n\n    # TODO how to memoize everything by default?\n    @staticmethod\n    @lru_cache(None)\n    def constant(value: numeric, name: str = None):\n        \"\"\"micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph\"\"\"\n        return Variable(value=value, name=name)\n\n    @staticmethod\n    def convert_input_into_variable(\n        operator_impl,\n    ) -&gt; Callable[[Any, ...], Sequence[Variable]]:\n        @functools.wraps(operator_impl)\n        def operator_wrapped(self, *raw_inputs: numeric | Variable):\n            if not TYPE_SAFE:\n                return operator_impl(self, *raw_inputs)\n            converted_args: list[Variable] = []\n            for raw_other in raw_inputs:\n                converted: Variable\n                match raw_other:\n                    case ndarr if isinstance(ndarr, ndarray):\n                        # TODO check supported dimensions\n                        converted_args.append(Variable(raw_other))\n                    case raw_numeric if isinstance(raw_numeric, (int, float)):\n                        converted_args.append(Variable.constant(raw_numeric))\n                    case already_variable if isinstance(already_variable, Variable):\n                        converted_args.append(already_variable)\n                    case _:\n                        raise TypeError(\n                            f\"Unsupported type for Variable operation: {type(raw_other).__name__}\"\n                        )\n            return operator_impl(self, *converted_args)\n\n        return operator_wrapped\n\n    def __repr__(self):\n        return f\"{self.name}(value={self.value})\"\n\n    # operators have no explicit type checks for micro optimization as static typechecks should be enough\n    @convert_input_into_variable\n    def __add__(self, other: Variable | numeric):\n        return operator_add(self, other)\n\n    def __radd__(self, other: Variable | numeric):\n        return self + other\n\n    @convert_input_into_variable\n    def __neg__(self, other: Variable | numeric):\n        return operator_neg(self)\n\n    @convert_input_into_variable\n    def __sub__(self, other: Variable | numeric):\n        return operator_add(self, -other)\n\n    @convert_input_into_variable\n    def __rsub__(self, other: Variable | numeric):\n        return operator_add(-other, self)\n\n    @convert_input_into_variable\n    def __mul__(self, other: Variable | numeric):\n        if TYPE_SAFE:\n            if self.shape != other.shape:\n                other = self.broadcast_to(self.shape)\n        return operator_mul(self, other)\n\n    def __rmul__(self, other: Variable | numeric):\n        # elementwise multiplication forced commutative\n        return self * other\n\n    @convert_input_into_variable\n    def __matmul__(self, other: Variable | ndarray):\n        if TYPE_SAFE:\n            if other.ndim == 1:\n                other = other.reshape_as_matrix()\n        return operator_matmul(self, other)\n\n    @convert_input_into_variable\n    def __rmatmul__(self, other: Variable | ndarray):\n        return operator_matmul(other, self)\n\n    def flip(self) -&gt; Variable:\n        return operator_flip(self)\n\n    def pad(self, pad_width: Sequence[tuple[int, int]]) -&gt; Variable:\n        return operator_pad(self, pad_width=pad_width)\n\n    def sum(self, axis: tuple[int, ...] | int | None = None, keepdims = False) -&gt; Variable:\n        return operator_sum(self, axis=axis, keepdims=keepdims)\n\n    def broadcast_to(self, shape) -&gt; Variable:\n        return operator_broadcast_to(self, broadcast_shap=shape)\n\n    def reshape(self, shape) -&gt; Variable:\n        return operator_reshape(self, new_shape=shape)\n\n    @property\n    def shape(self):\n        return self.value.shape\n\n    @property\n    def dtype(self):\n        return self.value.dtype\n\n    @property\n    def T(self) -&gt; Variable:\n        return operator_transpose(self)\n\n    @property\n    def ndim(self):\n        return self.value.ndim\n\n    @property\n    def size(self):\n        return self.value.size\n\n    @staticmethod\n    def convolve(matrix: Variable, kernel: Variable):\n        \"\"\"Convolve matrix with kernel, both\"\"\"\n        if TYPE_SAFE:\n            assert matrix.value.ndim == kernel.value.ndim, (\n                \"convolution arguments dimensionality must match.\"\n            )\n            assert (\n                matrix.value.shape[0] &gt; kernel.value.shape[0]\n                and matrix.value.shape[1] &gt; kernel.value.shape[1]\n            ), (\n                \"Kernel size must be smaller than input size for convolution. Use elementwise multiplication instead.\"\n            )\n\n        return operator_convolution(matrix, kernel)\n\n    @staticmethod\n    def ReLU(variable: Variable):\n        return activation_ReLU(variable)\n\n    @staticmethod\n    def sigmoid(variable: Variable):\n        return activation_sigmoid(variable)\n\n    @staticmethod\n    def tanh(variable: Variable):\n        return activation_tanh(variable)\n\n    def backward(\n        self, directional_grad: Variable | None = None\n    ) -&gt; dict[Variable, Variable]:\n        \"\"\"Convenience method to compute gradients of this variable with respect to all other variables in the graph.\"\"\"\n        global _tape_stack\n        return grad(self, custom_entry_gradient=directional_grad, tape_records=_tape_stack)\n</code></pre>"},{"location":"api/#src.autodiff.variable.Variable.backward","title":"backward","text":"<pre><code>backward(directional_grad: Variable | None = None) -&gt; dict[Variable, Variable]\n</code></pre> <p>Convenience method to compute gradients of this variable with respect to all other variables in the graph.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def backward(\n    self, directional_grad: Variable | None = None\n) -&gt; dict[Variable, Variable]:\n    \"\"\"Convenience method to compute gradients of this variable with respect to all other variables in the graph.\"\"\"\n    global _tape_stack\n    return grad(self, custom_entry_gradient=directional_grad, tape_records=_tape_stack)\n</code></pre>"},{"location":"api/#src.autodiff.variable.Variable.constant","title":"constant  <code>cached</code> <code>staticmethod</code>","text":"<pre><code>constant(value: numeric, name: str = None)\n</code></pre> <p>micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>@staticmethod\n@lru_cache(None)\ndef constant(value: numeric, name: str = None):\n    \"\"\"micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph\"\"\"\n    return Variable(value=value, name=name)\n</code></pre>"},{"location":"api/#src.autodiff.variable.Variable.convolve","title":"convolve  <code>staticmethod</code>","text":"<pre><code>convolve(matrix: Variable, kernel: Variable)\n</code></pre> <p>Convolve matrix with kernel, both</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>@staticmethod\ndef convolve(matrix: Variable, kernel: Variable):\n    \"\"\"Convolve matrix with kernel, both\"\"\"\n    if TYPE_SAFE:\n        assert matrix.value.ndim == kernel.value.ndim, (\n            \"convolution arguments dimensionality must match.\"\n        )\n        assert (\n            matrix.value.shape[0] &gt; kernel.value.shape[0]\n            and matrix.value.shape[1] &gt; kernel.value.shape[1]\n        ), (\n            \"Kernel size must be smaller than input size for convolution. Use elementwise multiplication instead.\"\n        )\n\n    return operator_convolution(matrix, kernel)\n</code></pre>"},{"location":"api/#src.autodiff.variable._ONES","title":"_ONES  <code>cached</code>","text":"<pre><code>_ONES(arr_shape: int | Iterable[int], dtype=None) -&gt; Variable\n</code></pre> <p>Returns cached Variable of ones with a given shape and type</p> <p>Thus this should never be a learnable parameter (must stay immutable)</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>@lru_cache(maxsize=None)\ndef _ONES(arr_shape: int | Iterable[int], dtype=None) -&gt; Variable:\n    \"\"\"\n    Returns cached Variable of ones with a given shape and type\n\n    Thus this should never be a learnable parameter (must stay immutable)\n    \"\"\"\n    return Variable(ones(arr_shape, dtype=dtype).view())\n</code></pre>"},{"location":"api/#src.autodiff.variable._ZEROS","title":"_ZEROS  <code>cached</code>","text":"<pre><code>_ZEROS(arr_shape: int | Iterable[int], dtype=None) -&gt; Variable\n</code></pre> <p>Returns cached Variable of zeros with a given shape and type</p> <p>Thus this should never be a learnable parameter (must stay immutable)</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>@lru_cache(maxsize=None)\ndef _ZEROS(arr_shape: int | Iterable[int], dtype=None) -&gt; Variable:\n    \"\"\"\n    Returns cached Variable of zeros with a given shape and type\n\n    Thus this should never be a learnable parameter (must stay immutable)\n    \"\"\"\n    return Variable(zeros(arr_shape, dtype=dtype).view())\n</code></pre>"},{"location":"api/#src.autodiff.variable._image2windows","title":"_image2windows","text":"<pre><code>_image2windows(image: ndarray, window_shape: shape) -&gt; ndarray\n</code></pre> <p>Extracts a view of sliding windows from the input image based on windws_shape.</p> <p>Assumes no padding and up/down sliding stride of 1.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def _image2windows(image: ndarray, window_shape: shape) -&gt; ndarray:\n    \"\"\"\n    Extracts a view of sliding windows from the input image based on windws_shape.\n\n    Assumes no padding and up/down sliding stride of 1.\n    \"\"\"\n    i_rows, i_cols = image.shape\n    w_rows, w_cols = window_shape\n    output_rows = i_rows - w_rows + 1\n    output_cols = i_cols - w_cols + 1\n\n    sliding_windows_shape = (output_rows, output_cols, w_rows, w_cols)\n    sliding_windows_strides = (\n        image.strides[0],\n        image.strides[1],\n        image.strides[0],\n        image.strides[1],\n    )\n\n    return as_strided(\n        image, shape=sliding_windows_shape, strides=sliding_windows_strides\n    )\n</code></pre>"},{"location":"api/#src.autodiff.variable._space2windows","title":"_space2windows","text":"<pre><code>_space2windows(space: ndarray, window_shape: shape) -&gt; ndarray\n</code></pre> <p>Extracts a view of sliding windows from the input space based on windws_shape. Assumes no padding and sliding stride of 1 in all dimensions.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def _space2windows(space: ndarray, window_shape: shape) -&gt; ndarray:\n    \"\"\"\n    Extracts a view of sliding windows from the input space based on windws_shape.\n    Assumes no padding and sliding stride of 1 in all dimensions.\n    \"\"\"\n    s_1d, s_2d, s_3d = space.shape\n    w_1d, w_2d, w_3d = window_shape\n    output_1d = s_1d - w_1d + 1\n    output_2d = s_2d - w_2d + 1\n    output_3d = s_3d - w_3d + 1\n\n    sliding_windows_shape = (output_1d, output_2d, output_3d, w_1d, w_2d, w_3d)\n    sliding_windows_strides = (\n        space.strides[0],\n        space.strides[1],\n        space.strides[2],\n        space.strides[0],\n        space.strides[1],\n        space.strides[2],\n    )\n\n    return as_strided(\n        space, shape=sliding_windows_shape, strides=sliding_windows_strides\n    )\n</code></pre>"},{"location":"api/#src.autodiff.variable._stream2windows","title":"_stream2windows","text":"<pre><code>_stream2windows(stream: ndarray, window_length: shape) -&gt; ndarray\n</code></pre> <p>Extracts a view of sliding windows from the input 1D stream based on window_length. Assumes no padding and left/right sliding stride of 1.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def _stream2windows(stream: ndarray, window_length: shape) -&gt; ndarray:\n    \"\"\"\n    Extracts a view of sliding windows from the input 1D stream based on window_length.\n    Assumes no padding and left/right sliding stride of 1.\n    \"\"\"\n    (stream_length,) = stream.shape\n    output_length = stream_length - window_length + 1\n\n    sliding_windows_shape = (output_length, window_length)\n    sliding_windows_strides = (stream.strides[0], stream.strides[0])\n\n    return as_strided(\n        stream, shape=sliding_windows_shape, strides=sliding_windows_strides\n    )\n</code></pre>"},{"location":"api/#src.autodiff.variable.activation_ReLU","title":"activation_ReLU","text":"<pre><code>activation_ReLU(var: Variable)\n</code></pre> <p>Element-wise Rectified Linear Unit activation function</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def activation_ReLU(var: Variable):\n    \"\"\"Element-wise Rectified Linear Unit activation function\"\"\"\n\n    forward = Variable(maximum(0, var.value))\n    inputs = (var,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dResult,) = dLoss_dOutputs\n        dLoss_dInput = dLoss_dResult * Variable((var.value &gt; 0).astype(float))\n        dLoss_dInputs = (dLoss_dInput,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.variable.activation_log_softmax","title":"activation_log_softmax","text":"<pre><code>activation_log_softmax(var: Variable, axis: int = -1)\n</code></pre> <p>Element-wise Log(Softmax(var)) activation function along the specified axis</p> <p>Use it with Negative-Log-Likelihood to achieve cross-entropy loss.</p> <p>Note that for regular softmax the derivatives would have looked like     dSoftmax_i/dInput_i = Softmax_i * (1 - Softmax_i)     dSoftmax_i/dInput_j = -Softmax_i * Softmax_j; if i != j however with log it simplifies to:     dLogSoftmax_i/dInput_i = 1 - Softmax_i     dLogSoftmax_i/dInput_j = -Softmax_j; if i != j</p> <p>This implementation includes the standard numerical stability trick with the max subtraction.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def activation_log_softmax(var: Variable, axis: int = -1):\n    \"\"\"\n    Element-wise Log(Softmax(var)) activation function along the specified axis\n\n    Use it with Negative-Log-Likelihood to achieve cross-entropy loss.\n\n    Note that for regular softmax the derivatives would have looked like\n        dSoftmax_i/dInput_i = Softmax_i * (1 - Softmax_i)\n        dSoftmax_i/dInput_j = -Softmax_i * Softmax_j; if i != j\n    however with log it simplifies to:\n        dLogSoftmax_i/dInput_i = 1 - Softmax_i\n        dLogSoftmax_i/dInput_j = -Softmax_j; if i != j\n\n    This implementation includes the standard numerical stability trick with the max subtraction.\n    \"\"\"\n\n    var_value = var.value\n    normalized_exp_values = exp(var_value - var_value.max(axis=axis, keepdims=True))\n    # log-sum-exp stability trick\n    log_softmax_values = normalized_exp_values - log(\n        normalized_exp_values.sum(axis=axis, keepdims=True)\n    )\n    forward = Variable(log_softmax_values)\n\n    inputs = (var,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dResult,) = dLoss_dOutputs\n        # Jacobian matrix for log_softmax is complex but gives more control over the learning process\n        dLoss_dInput = (\n            dLoss_dResult.broadcast_to((*var.shape, *dLoss_dResult.shape)) @ -forward\n            + dLoss_dResult\n        )\n        dLoss_dInputs = (dLoss_dInput,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.variable.activation_sigmoid","title":"activation_sigmoid","text":"<pre><code>activation_sigmoid(var: Variable)\n</code></pre> <p>Element-wise Sigmoid activation function</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def activation_sigmoid(var: Variable):\n    \"\"\"Element-wise Sigmoid activation function\"\"\"\n\n    forward = Variable(1.0 / (1.0 + exp(-var.value)))\n    inputs = (var,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dResult,) = dLoss_dOutputs\n        dLoss_dInput = dLoss_dResult * (forward * (1 - forward))\n        dLoss_dInputs = (dLoss_dInput,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.variable.activation_tanh","title":"activation_tanh","text":"<pre><code>activation_tanh(var: Variable)\n</code></pre> <p>Element-wise Tanh activation function</p> <p>tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def activation_tanh(var: Variable):\n    \"\"\"\n    Element-wise Tanh activation function\n\n    tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))\n    \"\"\"\n\n    forward = Variable(tanh(var.value))\n    inputs = (var,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dResult,) = dLoss_dOutputs\n        dLoss_dInput = dLoss_dResult * (1 - forward * forward)\n        dLoss_dInputs = (dLoss_dInput,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.variable.convolve_forward","title":"convolve_forward","text":"<pre><code>convolve_forward(tensor: ndarray, kernel: ndarray, optimize: Literal['greedy', 'optimal'] | bool | Sequence | None = True) -&gt; ndarray\n</code></pre> <p>Performs a convolution operation on the input ndarray with the given kernel.</p> <p>Matrix and Kernel must adhere to constraints. This library version supports 1d,2d and 3d convolutions, but more dimensions are straightforward to implement. Note that if the length of dimensions of tensor and kernel match, custom 4D, 5D, and so on sliding windows result in the exact same backward functions!</p> <p>This implementation does not flip the kernel, effectively performing a Convolution* a.k.a. Cross-Correlation. Note we do not perform padding and assume a stride of 1.</p> <p>The 'optimize' param is passed to the numpy.einsum for optimized broadcasted multiplication and reduction sum with no optimization the broadcasting with einsum is already faster because it doesn't create copies of intermediate arrays.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def convolve_forward(\n    tensor: ndarray,\n    kernel: ndarray,\n    optimize: Literal[\"greedy\", \"optimal\"] | bool | Sequence | None = True,\n) -&gt; ndarray:\n    \"\"\"\n    Performs a convolution operation on the input ndarray with the given kernel.\n\n    Matrix and Kernel must adhere to constraints.\n    This library version supports 1d,2d and 3d convolutions, but more dimensions are straightforward to implement.\n    Note that if the length of dimensions of tensor and kernel match, custom 4D, 5D, and so on sliding windows\n    result in the exact same backward functions!\n\n    This implementation does not flip the kernel, effectively performing a Convolution* a.k.a. Cross-Correlation.\n    Note we do not perform padding and assume a stride of 1.\n\n    The 'optimize' param is passed to the numpy.einsum for optimized broadcasted multiplication and reduction sum\n    with no optimization the broadcasting with einsum is already faster because it doesn't create copies of intermediate arrays.\n    \"\"\"\n    match kernel.shape:\n        case (_,):\n            windows = _stream2windows(tensor, kernel.shape)\n            subscript = \"ij,j-&gt;i\"\n        case (_, _):\n            windows = _image2windows(tensor, kernel.shape)\n            subscript = \"ijkl,kl-&gt;ij\"\n        case (_, _, _):\n            windows = _space2windows(tensor, kernel.shape)\n            subscript = \"ijklmn,lmn-&gt;ijk\"\n        case _:\n            raise ValueError(\"This type of convolution needs a custom implementation.\")\n\n    # broadcast multiplication and sum over the last kernel.ndim axis\n    return einsum(subscript, windows, kernel, optimize=optimize)\n</code></pre>"},{"location":"api/#src.autodiff.variable.get_tape_stack_snapshot","title":"get_tape_stack_snapshot","text":"<pre><code>get_tape_stack_snapshot() -&gt; list[Tape]\n</code></pre> <p>returns a snapshot shallow copy of the tape stack</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def get_tape_stack_snapshot() -&gt; list[Tape]:\n    \"\"\"returns a snapshot shallow copy of the tape stack\"\"\"\n    global _tape_stack\n    return _tape_stack[:]\n</code></pre>"},{"location":"api/#src.autodiff.variable.grad","title":"grad","text":"<pre><code>grad(loss_variable: Variable, *, tape_records=None, custom_entry_gradient: Variable | None = None) -&gt; dict[Variable, Variable]\n</code></pre> <p>Computes gradients of the loss_variable with respect to each Variable in the computation graph, dLoss_d lookup map effectively works out to make dLoss_d[x] equal to dLoss/dx</p> <p>:returns     a dictionary of the d(loss_variable)/d[key] where key is any other variable used to compute the loss_variable</p> <p>:params loss_variable:     The top node of the computation graph representing the loss.     If the loss is not a scalar, it's implicitly set to be an array of ones with the same shape as the loss.</p> desired_results <p>If the tape does not contain a variable, we consider its gradient None (which brings pruning of unused graoh branches to constant time checks).</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def grad(\n    loss_variable: Variable,\n    *,\n    tape_records=None,\n    custom_entry_gradient: Variable | None = None,\n    # desired_results: Sequence[Variable] | None = None,\n) -&gt; dict[Variable, Variable]:\n    \"\"\"\n    Computes gradients of the loss_variable with respect to each Variable in the computation graph,\n    dLoss_d lookup map effectively works out to make dLoss_d[x] equal to dLoss/dx\n\n    :returns\n        a dictionary of the d(loss_variable)/d[key] where key is any other variable used to compute the loss_variable\n\n    :params\n    loss_variable:\n        The top node of the computation graph representing the loss.\n        If the loss is not a scalar, it's implicitly set to be an array of ones with the same shape as the loss.\n\n    desired_results:\n        # TODO currently serves no optimization\n        Selects what we exclude from the gradient result list.\n        If desired_results is None, gradients for all variables in the computation graph will be computed.\n\n    If the tape does not contain a variable,\n    we consider its gradient None (which brings pruning of unused graoh branches to constant time checks).\n    \"\"\"\n\n    if tape_records is None:\n        tape_records = get_tape_stack_snapshot()\n    dLoss_d: dict[Variable, Variable] = {\n        loss_variable: _ONES(loss_variable.shape, loss_variable.dtype)\n        if custom_entry_gradient is None\n        else custom_entry_gradient\n    }\n\n    def prune_unused_outputs(\n        outputs: tuple[Variable, ...],\n    ) -&gt; tuple[Variable | None, ...]:\n        return tuple(\n            (dLoss_d[output] if output in dLoss_d else None) for output in outputs\n        )\n\n    for tape_record in reversed(tape_records):\n        dLoss_dOutputs = prune_unused_outputs(tape_record.outputs)\n\n        if all(dL_dOutput is None for dL_dOutput in dLoss_dOutputs):\n            continue  # prune paths equal to zero vectors\n\n        # perform chain rule back propagation\n        dLoss_dInputs = tape_record.back_fn(dLoss_dOutputs)\n\n        # due to unconstrained computation graph shape we actually sum all the gradients\n        # the automatic topological sorting of the tape ensures we use the final dLoss_d[tape_input] value\n        # only when it was fully computed\n        for tape_input, dL_dInput in zip(tape_record.inputs, dLoss_dInputs):\n            # we could have used defaultdict(lambda x:zeros) but this way we keep the notion of what was used in the process\n            if tape_input not in dLoss_d:\n                dLoss_d[tape_input] = dL_dInput\n            else:\n                dLoss_d[tape_input] += dL_dInput\n\n    # debug information values of each intermediate gradient\n    # for name, value in dLoss_d.items():\n    #     print(f\"d{loss_variable.name}_d{name} = {value.name}\")\n\n    return dLoss_d\n</code></pre>"},{"location":"api/#src.autodiff.variable.grad--todo-currently-serves-no-optimization","title":"TODO currently serves no optimization","text":"<p>Selects what we exclude from the gradient result list. If desired_results is None, gradients for all variables in the computation graph will be computed.</p>"},{"location":"api/#src.autodiff.variable.loss_NLL","title":"loss_NLL","text":"<pre><code>loss_NLL(log_probs: Variable, target_probs: Variable) -&gt; Variable\n</code></pre> <p>Negative Log-Likelihood Loss for classification tasks.</p> <p>:param log_probs: Log probabilities from the model (output of log_softmax). :param target_probs: True underlying probability indices (class labels). :return: NLL loss variable. Combined with log_softmax gives cross-entropy loss.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>@Variable.convert_input_into_variable\ndef loss_NLL(log_probs: Variable, target_probs: Variable) -&gt; Variable:\n    \"\"\"\n    Negative Log-Likelihood Loss for classification tasks.\n\n    :param log_probs: Log probabilities from the model (output of log_softmax).\n    :param target_probs: True underlying probability indices (class labels).\n    :return: NLL loss variable. Combined with log_softmax gives cross-entropy loss.\n    \"\"\"\n    n_samples = log_probs.value.shape[0]\n    nll_values = -log_probs.value[range(n_samples), target_indices]\n    nll_loss = Variable(nll_values.sum() * (1.0 / n_samples))\n    return nll_loss\n</code></pre>"},{"location":"api/#src.autodiff.variable.loss_mse","title":"loss_mse","text":"<pre><code>loss_mse(predicted: Variable, target: Variable | numeric | Iterable[numeric]) -&gt; Variable\n</code></pre> <p>Mean Squared Error Loss between predicted and target variables.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>@Variable.convert_input_into_variable\ndef loss_mse(\n    predicted: Variable, target: Variable | numeric | Iterable[numeric]\n) -&gt; Variable:\n    \"\"\"\n    Mean Squared Error Loss between predicted and target variables.\n    \"\"\"\n    n_samples = predicted.shape[0]\n\n    err = subtract(predicted.value, target.value)\n    mse_value = array(mean(square(err)))\n    mse = Variable(mse_value)\n    normalization_const = Variable.constant(1.0 / predicted.size)\n\n    inputs = (predicted,)  # the target is a constant, so skip it here\n    outputs = (mse,)\n    def back_fn(dLoss_dOutputs):\n        (dLoss_dOutput,) = dLoss_dOutputs\n        # ignore the factor of 2\n        dLoss_dInput = dLoss_dOutput * err * normalization_const.broadcast_to(predicted.shape),\n        return (dLoss_dInput,)\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return mse\n</code></pre>"},{"location":"api/#src.autodiff.variable.operator_broadcast_to","title":"operator_broadcast_to","text":"<pre><code>operator_broadcast_to(tensor: Variable, broadcast_shap) -&gt; Variable\n</code></pre> <p>Broadcasts the array variable to the given shape equivalent to numpy.broadcast_to</p> <p>Beware this function uses numpy's view under the hood so the result of this operator must remain immutable.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def operator_broadcast_to(tensor: Variable, broadcast_shap) -&gt; Variable:\n    \"\"\"\n    Broadcasts the array variable to the given shape\n    equivalent to numpy.broadcast_to\n\n    Beware this function uses numpy's view under the hood\n    so the result of this operator must remain immutable.\n    \"\"\"\n    forward = Variable(broadcast_to(tensor.value, shape=broadcast_shap))\n\n    inputs = (tensor,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dBroadcastedResult,) = dLoss_dOutputs\n\n        # sum over the broadcasted dimensions\n        reduce_extra_axes = tuple(\n            range(dLoss_dBroadcastedResult.ndim - tensor.value.ndim)\n        )\n        intermediate_sum = dLoss_dBroadcastedResult.sum(axis=reduce_extra_axes)\n        reduce_inner_axes = tuple(\n            axis\n            for axis, (dim1, dim2) in enumerate(\n                zip(tensor.value.shape, intermediate_sum.shape)\n            )\n            if dim1 != dim2\n        )\n        dLoss_dTensor = dLoss_dBroadcastedResult.sum(\n            axis=reduce_inner_axes, keepdims=True\n        )\n        if TYPE_SAFE:\n            assert dLoss_dTensor.shape == tensor.shape, (\n                \"shape mismatch in broadcast_to backward pass\"\n            )\n\n        dLoss_dInputs = (dLoss_dTensor,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.variable.operator_flip","title":"operator_flip","text":"<pre><code>operator_flip(matrix: Variable, **kwargs) -&gt; Variable\n</code></pre> <p>flip the array variable along dimension equivalent to numpy.flip</p> <p>operator_flip(array) is equivalent to array[::-1,::-1, ..., ::-1]</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def operator_flip(matrix: Variable, **kwargs) -&gt; Variable:\n    \"\"\"\n    flip the array variable along dimension\n    equivalent to numpy.flip\n\n    operator_flip(array) is equivalent to array[::-1,::-1, ..., ::-1]\n    \"\"\"\n    forward = Variable(flip(matrix.value, **kwargs))\n\n    inputs = (matrix,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dRotatedResult,) = dLoss_dOutputs\n        dLoss_dMatrix = dLoss_dRotatedResult.flip()\n        dLoss_dInputs = (dLoss_dMatrix,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.variable.operator_pad","title":"operator_pad","text":"<pre><code>operator_pad(matrix: Variable, pad_width: Sequence[tuple[int, int]]) -&gt; Variable\n</code></pre> <p>Pads the array variable according to pad_width pad_width ~ ((before(dim), after(dim)) for dim in each dimension)</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def operator_pad(matrix: Variable, pad_width: Sequence[tuple[int, int]]) -&gt; Variable:\n    \"\"\"\n    Pads the array variable according to pad_width\n    pad_width ~ ((before(dim), after(dim)) for dim in each dimension)\n    \"\"\"\n    forward = Variable(pad(matrix.value, pad_width=pad_width))\n\n    inputs = (matrix,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dPaddedResult,) = dLoss_dOutputs\n\n        # slices = tuple(\n        #     slice(pad_width_dim[0], dLoss_dPaddedResult.value.shape[i] - pad_width_dim[1])\n        #     for i, pad_width_dim in enumerate(pad_width)\n        # )\n        # dLoss_dMatrix = Variable(dLoss_dPaddedResult.value[slices])\n\n        dLoss_dMatrix = matrix  # variables are immutable during grad computation, so only link dL_dIn matrix with dLdOut matrix padded with constants\n        dLoss_dInputs = (dLoss_dMatrix,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.variable.operator_reshape","title":"operator_reshape","text":"<pre><code>operator_reshape(tensor: Variable, new_shape) -&gt; Variable\n</code></pre> <p>Reshape tensor using numpy.reshape</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def operator_reshape(tensor: Variable, new_shape) -&gt; Variable:\n    \"\"\"\n    Reshape tensor using numpy.reshape\n    \"\"\"\n    forward = Variable(tensor.value.reshape(new_shape))\n    inputs = (tensor,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dResult,) = dLoss_dOutputs\n        dLoss_dInput = dLoss_dResult.reshape(tensor.shape)\n        dLoss_dInputs = (dLoss_dInput,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.variable.operator_sum","title":"operator_sum","text":"<pre><code>operator_sum(tensor: Variable, axis: tuple[int, ...] | int | None = None, keepdims: bool = False) -&gt; Variable\n</code></pre> <p>Sums the elements of the tensor along the specified axis. If axis is None, sums all elements.</p> Source code in <code>src/autodiff/variable.py</code> <pre><code>def operator_sum(\n    tensor: Variable,\n    axis: tuple[int, ...] | int | None = None,\n    keepdims: bool = False,\n) -&gt; Variable:\n    \"\"\"\n    Sums the elements of the tensor along the specified axis.\n    If axis is None, sums all elements.\n    \"\"\"\n    forward = Variable(tensor.value.sum(axis=axis, keepdims=keepdims))\n\n    inputs = (tensor,)\n    outputs = (forward,)\n\n    def back_fn(dLoss_dOutputs: Sequence[Variable]) -&gt; Sequence[Variable]:\n        (dLoss_dSumResult,) = dLoss_dOutputs\n        # without expand_dims this may crash, but it is no bug, we disallow such broadcasting in this version\n        dLoss_dTensor = dLoss_dSumResult.broadcast_to(tensor.shape)\n        dLoss_dInputs = (dLoss_dTensor,)\n        return dLoss_dInputs\n\n    global _tape_stack\n    _tape_stack.append(Tape(outputs=outputs, inputs=inputs, back_fn=back_fn))\n\n    return forward\n</code></pre>"},{"location":"api/#src.autodiff.tape.Tape","title":"Tape","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Single tape record containing information about a single computation step in the graph</p> <p>Remark that the back_fn callback operates on Variable instances that entails that a new graph is being built during the backward pass</p> Source code in <code>src/autodiff/tape.py</code> <pre><code>class Tape(NamedTuple):\n    \"\"\"\n    Single tape record containing information about a single computation step in the graph\n\n    Remark that the back_fn callback operates on Variable instances\n    that entails that a new graph is being built during the backward pass\n    \"\"\"\n\n    outputs: Sequence[Variable]\n    inputs: Sequence[Variable]\n    # back_fn: BackFunction\n    back_fn: Callable[[Sequence[Variable | None]], Sequence[Variable]]\n</code></pre>"},{"location":"api/#src.network.network.Linear","title":"Linear","text":"<p>               Bases: <code>Network</code></p> <p>Ready to use linear subnetwork implementation equivalent to Wx+b linear step</p> Source code in <code>src/network/network.py</code> <pre><code>class Linear(Network):\n    \"\"\"\n    Ready to use linear subnetwork implementation\n    equivalent to Wx+b linear step\n    \"\"\"\n    def __init__(self, in_features_length: int, out_features_length: int):\n        super().__init__()\n        self.weight = Variable(random((out_features_length, in_features_length)))\n        self.bias = Variable(\n            random(\n                out_features_length,\n            )\n        )\n\n    def forward(self, x: Variable) -&gt; Variable:\n        # todo reduce 2 operations into one by special operator_lin_step(w, b, x)\n        alpha = self.weight @ x + self.bias\n        return alpha\n</code></pre>"},{"location":"api/#src.network.network.Network","title":"Network","text":"<p>               Bases: <code>ABC</code></p> <p>Modular neural network base class.</p> <p>To use, inherit from Network e.g.: FooNN(Network):     init(self, ...):         super().init()         declared here fields either of type Variable or Network         will be stored marked as learnable parameters         ...     forward(self, ...):         ...         overwrite of abstract forward is being required         this function needs to return the output Variable</p> Source code in <code>src/network/network.py</code> <pre><code>class Network(abc.ABC):\n    \"\"\"\n    Modular neural network base class.\n\n    To use, inherit from Network e.g.:\n    FooNN(Network):\n        __init__(self, ...):\n            super().__init__()\n            declared here fields either of type Variable or Network\n            will be stored marked as learnable parameters\n            ...\n        forward(self, ...):\n            ...\n            overwrite of abstract forward is being required\n            this function needs to return the output Variable\n    \"\"\"\n\n    __slots__ = (\"__dict__\", \"_parameters\", \"_subnetworks\")\n\n    def __init__(self):\n        object.__setattr__(self, \"_parameters\", [])\n        object.__setattr__(self, \"_subnetworks\", [])\n\n    def __setattr__(self, name, value):\n        parameters = getattr(self, \"_parameters\", None)\n        subnetworks = getattr(self, \"_subnetworks\", None)\n\n        if parameters is None or subnetworks is None:\n            raise AttributeError(\n                f\"Cannot assign attributes to Network before {self.__class__.__name__}.__init__() initialization.\"\n            )\n\n        match value:\n            case Variable():\n                parameters.append(value)\n                if name in subnetworks:\n                    del subnetworks[name]\n            case Network():\n                subnetworks.append(value)\n                if name in parameters:\n                    del parameters[name]\n\n        object.__setattr__(self, name, value)\n\n    @abstractmethod\n    def forward(self, *x: Variable) -&gt; Sequence[Variable] | Variable:\n        raise NotImplementedError\n\n    def __call__(self, *x: Variable) -&gt; Sequence[Variable] | Variable:\n        return self.forward(*x)\n\n    def parameters(self, recurse=True) -&gt; Iterable[Variable]:\n        return chain(\n            self._parameters,\n            (\n                param\n                for subnetwork in self._subnetworks\n                for param in subnetwork.parameters(recurse=recurse)\n            ),\n        )\n</code></pre>"}]}