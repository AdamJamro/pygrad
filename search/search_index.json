{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"api/","title":"API Reference","text":"<p>PyGrad - A neural network library with autograd automatic gradient calculation.</p> <p>@Author Adam Jamrozi\u0144ski</p> <p>Variable is the building block of the computation graph each overloaded or custom operator saves itself as a record on a tape stack</p> <p>The _tape_stack is bundled with the variable module by default. It contains all the information about which variable was needed to compute the other. The information how to compute the backward partial derivative at each step. Thanks to this approach, the list is being automatically sorted topologically (in reverse order)</p> <p>Each variable in the computation graph can be passed to the grad(...) function to compute its derivative</p> <p>The backward pass is computed via a topologically sorted stack of callbacks aka the backward pass is \"tape-based\". Tape class organizes and singles out Variable's responsibility of storing the computation graph (DAG). Additionally, it is a neat way we topologically sort the computation graph, since the computation is going to be performed in the same order we put it on the tape.</p>"},{"location":"api/#src.Variable","title":"Variable","text":""},{"location":"api/#src.Variable.backward","title":"backward","text":"<pre><code>backward(directional_grad: Variable | None = None) -&gt; dict[Variable, ndarray]\n</code></pre> <p>Convenience method to compute gradients of this variable with respect to all other variables in the graph.</p>"},{"location":"api/#src.Variable.constant","title":"constant  <code>cached</code> <code>staticmethod</code>","text":"<pre><code>constant(value: numeric, name: str = None)\n</code></pre> <p>micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph</p>"},{"location":"api/#src.Variable.convolve","title":"convolve  <code>staticmethod</code>","text":"<pre><code>convolve(matrix: Variable, kernel: Variable)\n</code></pre> <p>Convolve matrix with kernel, both</p>"},{"location":"api/#src.Variable.log_softmax","title":"log_softmax  <code>staticmethod</code>","text":"<pre><code>log_softmax(variable: Variable, axis: int = -1) -&gt; Variable\n</code></pre> <p>Computes log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x))) see the variable.activation_log_softmax() for more info</p>"},{"location":"api/#src.autodiff.variable.Variable","title":"Variable","text":""},{"location":"api/#src.autodiff.variable.Variable.backward","title":"backward","text":"<pre><code>backward(directional_grad: Variable | None = None) -&gt; dict[Variable, ndarray]\n</code></pre> <p>Convenience method to compute gradients of this variable with respect to all other variables in the graph.</p>"},{"location":"api/#src.autodiff.variable.Variable.constant","title":"constant  <code>cached</code> <code>staticmethod</code>","text":"<pre><code>constant(value: numeric, name: str = None)\n</code></pre> <p>micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph</p>"},{"location":"api/#src.autodiff.variable.Variable.convolve","title":"convolve  <code>staticmethod</code>","text":"<pre><code>convolve(matrix: Variable, kernel: Variable)\n</code></pre> <p>Convolve matrix with kernel, both</p>"},{"location":"api/#src.autodiff.variable.Variable.log_softmax","title":"log_softmax  <code>staticmethod</code>","text":"<pre><code>log_softmax(variable: Variable, axis: int = -1) -&gt; Variable\n</code></pre> <p>Computes log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x))) see the variable.activation_log_softmax() for more info</p>"},{"location":"api/#src.autodiff.variable._ONES","title":"_ONES  <code>cached</code>","text":"<pre><code>_ONES(arr_shape: int | Iterable[int], dtype=None) -&gt; Variable\n</code></pre> <p>Returns cached Variable of ones with a given shape and type</p> <p>Thus this should never be a learnable parameter (must stay immutable)</p>"},{"location":"api/#src.autodiff.variable._ZEROS","title":"_ZEROS  <code>cached</code>","text":"<pre><code>_ZEROS(arr_shape: int | Iterable[int], dtype=None) -&gt; Variable\n</code></pre> <p>Returns cached Variable of zeros with a given shape and type</p> <p>Thus this should never be a learnable parameter (must stay immutable)</p>"},{"location":"api/#src.autodiff.variable._image2windows","title":"_image2windows","text":"<pre><code>_image2windows(image: ndarray, window_shape: shape) -&gt; ndarray\n</code></pre> <p>Extracts a view of sliding windows from the input image based on windws_shape.</p> <p>Assumes no padding and up/down sliding stride of 1.</p>"},{"location":"api/#src.autodiff.variable._space2windows","title":"_space2windows","text":"<pre><code>_space2windows(space: ndarray, window_shape: shape) -&gt; ndarray\n</code></pre> <p>Extracts a view of sliding windows from the input space based on windws_shape. Assumes no padding and sliding stride of 1 in all dimensions.</p>"},{"location":"api/#src.autodiff.variable._stream2windows","title":"_stream2windows","text":"<pre><code>_stream2windows(stream: ndarray, window_length: shape) -&gt; ndarray\n</code></pre> <p>Extracts a view of sliding windows from the input 1D stream based on window_length. Assumes no padding and left/right sliding stride of 1.</p>"},{"location":"api/#src.autodiff.variable.activation_ReLU","title":"activation_ReLU","text":"<pre><code>activation_ReLU(var: Variable)\n</code></pre> <p>Element-wise Rectified Linear Unit activation function</p>"},{"location":"api/#src.autodiff.variable.activation_log_softmax","title":"activation_log_softmax","text":"<pre><code>activation_log_softmax(var: Variable, axis: int = -1)\n</code></pre> <p>Element-wise Log(Softmax(var)) activation function along the specified axis</p> <p>Use it with Negative-Log-Likelihood to achieve cross-entropy loss.</p> <p>Note that for regular softmax the derivatives would have looked like     dSoftmax_i/dInput_i = Softmax_i * (1 - Softmax_i)     dSoftmax_i/dInput_j = -Softmax_i * Softmax_j; if i != j however with log it simplifies to:     dLogSoftmax_i/dInput_i = 1 - Softmax_i     dLogSoftmax_i/dInput_j = -Softmax_j; if i != j</p> <p>This implementation includes the standard numerical stability trick with the max subtraction.</p>"},{"location":"api/#src.autodiff.variable.activation_sigmoid","title":"activation_sigmoid","text":"<pre><code>activation_sigmoid(var: Variable)\n</code></pre> <p>Element-wise Sigmoid activation function</p>"},{"location":"api/#src.autodiff.variable.activation_tanh","title":"activation_tanh","text":"<pre><code>activation_tanh(var: Variable)\n</code></pre> <p>Element-wise Tanh activation function</p> <p>tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))</p>"},{"location":"api/#src.autodiff.variable.convolve_forward","title":"convolve_forward","text":"<pre><code>convolve_forward(tensor: ndarray, kernel: ndarray, optimize: Literal['greedy', 'optimal'] | bool | Sequence | None = True) -&gt; ndarray\n</code></pre> <p>Performs a convolution operation on the input ndarray with the given kernel.</p> <p>Matrix and Kernel must adhere to constraints. This library version supports 1d,2d and 3d convolutions, but more dimensions are straightforward to implement. Note that if the length of dimensions of tensor and kernel match, custom 4D, 5D, and so on sliding windows result in the exact same backward functions!</p> <p>This implementation does not flip the kernel, effectively performing a Convolution* a.k.a. Cross-Correlation. Note we do not perform padding and assume a stride of 1.</p> <p>The 'optimize' param is passed to the numpy.einsum for optimized broadcasted multiplication and reduction sum with no optimization the broadcasting with einsum is already faster because it doesn't create copies of intermediate arrays.</p>"},{"location":"api/#src.autodiff.variable.get_tape_stack_snapshot","title":"get_tape_stack_snapshot","text":"<pre><code>get_tape_stack_snapshot() -&gt; list[Tape]\n</code></pre> <p>returns a snapshot shallow copy of the tape stack</p>"},{"location":"api/#src.autodiff.variable.grad","title":"grad","text":"<pre><code>grad(loss_variable: Variable, *, tape_records=None, custom_entry_gradient: Variable | None = None, desired_results: Sequence[Variable] | None = None) -&gt; dict[Variable, Variable]\n</code></pre> <p>Computes gradients of the loss_variable with respect to each Variable in the computation graph, dLoss_d lookup map effectively works out to make dLoss_d[x] equal to dLoss/dx</p> <p>:returns     a dictionary of the d(loss_variable)/d[key] where key is any other variable used to compute the loss_variable</p> <p>:params loss_variable:     The top node of the computation graph representing the loss.     If the loss is not a scalar, it's implicitly set to be an array of ones with the same shape as the loss.</p> desired_results <p>If the tape does not contain a variable, we consider its gradient None (which brings pruning of unused graoh branches to constant time checks).</p>"},{"location":"api/#src.autodiff.variable.grad--todo-currently-serves-no-optimization","title":"TODO currently serves no optimization","text":"<p>Selects what we exclude from the gradient result list. If desired_results is None, gradients for all variables in the computation graph will be computed.</p>"},{"location":"api/#src.autodiff.variable.loss_NLL","title":"loss_NLL","text":"<pre><code>loss_NLL(log_probs: Variable, target_probs: Variable) -&gt; Variable\n</code></pre> <p>Negative Log-Likelihood Loss for classification tasks.</p> <p>:param log_probs: Log probabilities from the model (output of log_softmax). :param target_probs: True underlying probability indices (class labels). :return: NLL loss variable. Combined with log_softmax gives cross-entropy loss.</p>"},{"location":"api/#src.autodiff.variable.loss_mae","title":"loss_mae","text":"<pre><code>loss_mae(predicted: Variable, target: Variable | numeric | Iterable[numeric]) -&gt; Variable\n</code></pre> <p>Mean Absolute Error Loss between predicted and target variables.</p>"},{"location":"api/#src.autodiff.variable.loss_mse","title":"loss_mse","text":"<pre><code>loss_mse(predicted: Variable, target: Variable | numeric | Iterable[numeric]) -&gt; Variable\n</code></pre> <p>Mean Squared Error Loss between predicted and target variables.</p>"},{"location":"api/#src.autodiff.variable.operator_broadcast_to","title":"operator_broadcast_to","text":"<pre><code>operator_broadcast_to(tensor: Variable, broadcast_shape) -&gt; Variable\n</code></pre> <p>Broadcasts the array variable to the given shape equivalent to numpy.broadcast_to</p> <p>Beware this function uses numpy's view under the hood so the result of this operator must remain immutable.</p>"},{"location":"api/#src.autodiff.variable.operator_flip","title":"operator_flip","text":"<pre><code>operator_flip(matrix: Variable, **kwargs) -&gt; Variable\n</code></pre> <p>flip the array variable along dimension equivalent to numpy.flip</p> <p>operator_flip(array) is equivalent to array[::-1,::-1, ..., ::-1]</p>"},{"location":"api/#src.autodiff.variable.operator_pad","title":"operator_pad","text":"<pre><code>operator_pad(matrix: Variable, pad_width: Sequence[tuple[int, int]]) -&gt; Variable\n</code></pre> <p>Pads the array variable according to pad_width pad_width ~ ((before(dim), after(dim)) for dim in each dimension)</p>"},{"location":"api/#src.autodiff.variable.operator_reshape","title":"operator_reshape","text":"<pre><code>operator_reshape(tensor: Variable, new_shape) -&gt; Variable\n</code></pre> <p>Reshape tensor using numpy.reshape</p>"},{"location":"api/#src.autodiff.variable.operator_sum","title":"operator_sum","text":"<pre><code>operator_sum(tensor: Variable, axis: tuple[int, ...] | int | None = None, keepdims: bool = False) -&gt; Variable\n</code></pre> <p>Sums the elements of the tensor along the specified axis. If axis is None, sums all elements.</p>"},{"location":"api/#src.autodiff.tape.Tape","title":"Tape","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Single tape record containing information about a single computation step in the graph</p> <p>Remark that the back_fn callback operates on Variable instances that entails that a new graph is being built during the backward pass</p>"},{"location":"api/#src.network.network.Linear","title":"Linear","text":"<p>               Bases: <code>Network</code></p> <p>Ready to use linear subnetwork implementation equivalent to Wx+b linear step</p>"},{"location":"api/#src.network.network.Network","title":"Network","text":"<p>               Bases: <code>ABC</code></p> <p>Modular neural network base class.</p> <p>To use, inherit from Network e.g.: FooNN(Network):     init(self, ...):         super().init()         declared here fields either of type Variable or Network         will be stored marked as learnable parameters         ...     forward(self, ...):         ...         overwrite of abstract forward is being required         this function needs to return the output Variable</p>"}]}