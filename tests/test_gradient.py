"""
@Author Adam Jamrozi≈Ñski

Tests for the variable automatic differentiation engine
"""

import math
import random
from functools import partial

import numpy
import pytest
from numpy import allclose

from src.autodiff.variable import Variable, clear_tape, grad, reset_name_ids
from tests.conftest import does_not_contribute, numeric_derivative

random.seed(random.randint(0, 10000))
numpy.random.seed(random.randint(0, 10000))


@pytest.fixture(autouse=True)
def empty_tape_stack(monkeypatch):
    """Ensure the tape is empty before each test."""

    clear_tape()
    yield
    clear_tape()
    reset_name_ids()


_float_whole_number_pairs = [
    (0.0, 1.0),
    (1.0, 1.0),
    (0.0, 0.0),
]

_float_whole_number_pairs.extend(
    [(random.randint(-1000, 1000), random.randint(-1000, 1000)) for _ in range(4)]
)

_float_residues = [0.001, 0.233, 0.5, 0.777, 0.999, 1e-10]
_float_pairs = _float_whole_number_pairs[:]
_float_pairs.extend(
    (x + res * i, y + res * j)
    for x, y in _float_whole_number_pairs
    for res in _float_residues
    for i in [-1, 0, 1]
    for j in [-1, 0, 1]
)  # all combinations


class TestValueBasicOperations:
    """Test basic arithmetic operations."""

    def test_addition_1st_gradient(self):
        """Test addition and gradient propagation."""
        a = Variable(2.0)
        b = Variable(3.0)
        c = a + b
        # dc_d = grad(c)
        dc_d = c.backward()

        assert c.value == 5.0
        assert dc_d[b].value == 1.0
        assert dc_d[a].value == 1.0
        assert dc_d[c].value == 1.0

    def test_2nd_gradient_None(self):
        """Test addition and gradient propagation."""
        a = Variable(2.0)
        b = Variable(3.0)
        c = a + b

        dc_d = grad(c)
        ddcda_d = grad(dc_d[a])

        assert c.value == 5.0
        assert dc_d[b].value == 1.0
        assert dc_d[a].value == 1.0
        assert dc_d[c].value == 1.0
        assert does_not_contribute(ddcda_d.get(a))

    def test_2nd_gradient_not_None(self):
        a = Variable(2.0)
        b = Variable(3.0)
        c = a + b
        loss = c * b
        dloss_d = grad(loss)
        ddlossdb_d = grad(dloss_d[b])
        assert dloss_d[b] in ddlossdb_d.keys()
        assert b in ddlossdb_d.keys()
        assert (
            ddlossdb_d[b].value == 2.0
        )  # d^2loss/db^2 = d( dloss/db )/db = d( a + 2b )/db = 2

    @pytest.mark.parametrize("a_value,b_value", [*_float_pairs])
    @pytest.mark.parametrize(
        "test_id, binary_operation",
        [
            (
                "addition",
                lambda a, b: a + b,
            ),
            (
                "multiplication",
                lambda a, b: a * b,
            ),
        ],
    )
    def test_simple_binary_operations(
        self, test_id, binary_operation, a_value, b_value
    ):
        """Test multiplication and gradient propagation."""
        a = Variable(a_value)
        b = Variable(b_value)
        c = binary_operation(a, b)
        dc_d = grad(c)

        assert allclose(c.value, binary_operation(a_value, b_value))
        assert allclose(
            numeric_derivative(lambda x: binary_operation(x, b_value), a_value),
            dc_d[a].value,
        )
        assert allclose(
            numeric_derivative(lambda x: binary_operation(a_value, x), b_value),
            dc_d[b].value,
        )

    @staticmethod
    def custom_function_0(a, b):
        t = a + b
        return t * b

    @staticmethod
    def custom_function_1(a, b):
        t1 = a * b
        t2 = a + t1
        return 2 * t1 + t2

    @pytest.mark.parametrize(
        "custom_function",
        [
            custom_function_0,
            custom_function_1,
        ],
    )
    def test_simple_compound_graph(self, custom_function):
        a_value = 5
        b_value = 6
        a = Variable(a_value)
        b = Variable(b_value)
        loss = custom_function(a, b)
        dloss_d = grad(loss)
        assert allclose(loss.value, custom_function(a_value, b_value))
        assert allclose(
            numeric_derivative(partial(custom_function, b=b_value), a_value),
            dloss_d[a].value,
        )

    # tests below are autogenerated are not implemented yet
    def test_power(self):
        """Test power operation and gradient propagation."""
        a = Value(2.0)
        b = a**3
        grad(b)

        assert b.data == 8.0
        assert a.grad == 12.0  # d/dx(x^3) = 3x^2 = 3*4 = 12

    def test_division(self):
        """Test division operation."""
        a = Value(6.0)
        b = Value(2.0)
        c = a / b
        grad(c)

        assert c.data == 3.0
        assert abs(a.grad - 0.5) < 1e-6
        assert abs(b.grad - (-1.5)) < 1e-6

    def test_subtraction(self):
        """Test subtraction operation."""
        a = Variable(5.0)
        b = Variable(3.0)
        c = a - b
        grad(c)

        assert c.data == 2.0
        assert a.grad == 1.0
        assert b.grad == -1.0

    def test_negation(self):
        """Test negation operation."""
        a = Variable(5.0)
        b = -a
        grad(b)

        assert b.data == -5.0
        assert a.grad == -1.0


class TestVariableReverseOperations:
    """Test reverse operations (e.g., 2 + Variable(3))."""

    def test_radd(self):
        """Test reverse addition."""
        a = Variable(3.0)
        b = 2.0 + a
        grad(b)

        assert b.data == 5.0
        assert a.grad == 1.0

    def test_rmul(self):
        """Test reverse multiplication."""
        a = Variable(3.0)
        b = 2.0 * a
        grad(b)

        assert b.data == 6.0
        assert a.grad == 2.0

    def test_rsub(self):
        """Test reverse subtraction."""
        a = Variable(3.0)
        b = 5.0 - a
        grad(b)

        assert b.data == 2.0
        assert a.grad == -1.0

    def test_rtruediv(self):
        """Test reverse division."""
        a = Variable(2.0)
        b = 6.0 / a
        grad(b)

        assert b.data == 3.0
        assert abs(a.grad - (-1.5)) < 1e-6


class TestActivationFunctions:
    """Test activation functions."""

    def test_relu_positive(self):
        """Test ReLU with positive input."""
        a = Variable(2.0)
        b = a.relu()
        grad(b)

        assert b.data == 2.0
        assert a.grad == 1.0

    def test_relu_negative(self):
        """Test ReLU with negative input."""
        a = Variable(-2.0)
        b = a.relu()
        grad(b)

        assert b.data == 0.0
        assert a.grad == 0.0

    def test_tanh(self):
        """Test tanh activation."""
        a = Variable(0.5)
        b = a.tanh()
        grad(b)

        expected = math.tanh(0.5)
        assert abs(b.data - expected) < 1e-6
        # Gradient of tanh is 1 - tanh^2
        expected_grad = 1 - expected**2
        assert abs(a.grad - expected_grad) < 1e-6

    def test_sigmoid(self):
        """Test sigmoid activation."""
        a = Variable(0.5)
        b = a.sigmoid()
        grad(b)

        expected = 1 / (1 + math.exp(-0.5))
        assert abs(b.data - expected) < 1e-6
        # Gradient of sigmoid is sigmoid * (1 - sigmoid)
        expected_grad = expected * (1 - expected)
        assert abs(a.grad - expected_grad) < 1e-6

    def test_exp(self):
        """Test exponential function."""
        a = Variable(2.0)
        b = a.exp()
        grad(b)

        expected = math.exp(2.0)
        assert abs(b.data - expected) < 1e-6
        assert abs(a.grad - expected) < 1e-6

    def test_log(self):
        """Test natural logarithm."""
        a = Variable(2.0)
        b = a.log()
        grad(b)

        expected = math.log(2.0)
        assert abs(b.data - expected) < 1e-6
        assert abs(a.grad - 0.5) < 1e-6  # d/dx(log(x)) = 1/x = 1/2


class TestComplexComputations:
    """Test more complex computational graphs."""

    def test_chain_rule(self):
        """Test chain rule with nested operations."""
        x = Variable(2.0)
        y = x * 2 + 3
        z = y**2
        grad(z)

        # z = (2x + 3)^2 = (2*2 + 3)^2 = 7^2 = 49
        assert z.data == 49.0
        # dz/dx = 2 * (2x + 3) * 2 = 4 * 7 = 28
        assert x.grad == 28.0

    def test_multiple_paths(self):
        """Test gradient accumulation through multiple paths."""
        x = Variable(3.0)
        y = x * x  # y = x^2
        z = y + x  # z = x^2 + x
        grad(z)

        # z = x^2 + x = 9 + 3 = 12
        assert z.data == 12.0
        # dz/dx = 2x + 1 = 6 + 1 = 7
        assert x.grad == 7.0

    def test_complex_expression(self):
        """Test a more complex expression."""
        a = Variable(2.0)
        b = Variable(3.0)
        c = Variable(-1.0)

        # f = (a * b + c) * (a + b * c)
        d = a * b + c
        e = a + b * c
        f = d * e
        grad(f)

        # d = 2*3 + (-1) = 5
        # e = 2 + 3*(-1) = -1
        # f = 5 * (-1) = -5
        assert f.data == -5.0

        # df/da = b * e + d * 1 = 3*(-1) + 5*1 = 2
        assert a.grad == 2.0
        # df/db = a * e + d * c = 2*(-1) + 5*(-1) = -7
        assert b.grad == -7.0
        # df/dc = 1 * e + d * b = 1*(-1) + 5*3 = 14
        assert c.grad == 14.0

    def test_zero_grad(self):
        """Test gradient zeroing."""
        a = Variable(2.0)
        b = a * 3
        grad(b)

        assert a.grad == 3.0

        a.zero_grad()
        assert a.grad == 0.0


class TestBackwardPropagation:
    """Test backward propagation mechanics."""

    def test_multiple_backward_calls(self):
        """Test that multiple backward calls accumulate gradients."""
        x = Variable(2.0)
        y = x * 3
        grad(y)

        first_grad = x.grad
        assert first_grad == 3.0

        # Call backward again - gradients should accumulate
        grad(y)
        assert x.grad == first_grad + 3.0  # 6.0

    def test_topological_sort(self):
        """Test that topological sort works correctly."""
        a = Variable(2.0)
        b = Variable(3.0)
        c = a + b
        d = c * 2
        e = d + a
        grad(e)

        # This tests that gradients flow correctly through the graph
        assert e.data == 12.0  # ((2+3)*2) + 2 = 12
        # de/da includes both direct path and path through c,d
        # de/da = 1 (direct) + 2 (through c and d)
        assert a.grad == 3.0
