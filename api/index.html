
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A Python machine learning library with automatic differentiation">
      
      
        <meta name="author" content="Adam Jamrozisnki">
      
      
      
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>API Reference - Autograd</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#src" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Autograd" class="md-header__button md-logo" aria-label="Autograd" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Autograd
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API Reference
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/adamJamro/pygrad" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    autograd
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  API Reference

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Autograd" class="md-nav__button md-logo" aria-label="Autograd" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Autograd
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/adamJamro/pygrad" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    autograd
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    API Reference
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    API Reference
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#src" class="md-nav__link">
    <span class="md-ellipsis">
      
        src
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.Variable" class="md-nav__link">
    <span class="md-ellipsis">
      
        Variable
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Variable">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.Variable.backward" class="md-nav__link">
    <span class="md-ellipsis">
      
        backward
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.Variable.constant" class="md-nav__link">
    <span class="md-ellipsis">
      
        constant
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.Variable.convolve" class="md-nav__link">
    <span class="md-ellipsis">
      
        convolve
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.Variable.log_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      
        log_softmax
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable" class="md-nav__link">
    <span class="md-ellipsis">
      
        variable
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.Variable" class="md-nav__link">
    <span class="md-ellipsis">
      
        Variable
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Variable">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.autodiff.variable.Variable.backward" class="md-nav__link">
    <span class="md-ellipsis">
      
        backward
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.autodiff.variable.Variable.constant" class="md-nav__link">
    <span class="md-ellipsis">
      
        constant
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.autodiff.variable.Variable.convolve" class="md-nav__link">
    <span class="md-ellipsis">
      
        convolve
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.autodiff.variable.Variable.log_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      
        log_softmax
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable._ONES" class="md-nav__link">
    <span class="md-ellipsis">
      
        _ONES
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable._ZEROS" class="md-nav__link">
    <span class="md-ellipsis">
      
        _ZEROS
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable._image2windows" class="md-nav__link">
    <span class="md-ellipsis">
      
        _image2windows
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable._space2windows" class="md-nav__link">
    <span class="md-ellipsis">
      
        _space2windows
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable._stream2windows" class="md-nav__link">
    <span class="md-ellipsis">
      
        _stream2windows
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.activation_ReLU" class="md-nav__link">
    <span class="md-ellipsis">
      
        activation_ReLU
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.activation_log_softmax" class="md-nav__link">
    <span class="md-ellipsis">
      
        activation_log_softmax
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.activation_sigmoid" class="md-nav__link">
    <span class="md-ellipsis">
      
        activation_sigmoid
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.activation_tanh" class="md-nav__link">
    <span class="md-ellipsis">
      
        activation_tanh
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.convolve_forward" class="md-nav__link">
    <span class="md-ellipsis">
      
        convolve_forward
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.get_tape_stack_snapshot" class="md-nav__link">
    <span class="md-ellipsis">
      
        get_tape_stack_snapshot
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.grad" class="md-nav__link">
    <span class="md-ellipsis">
      
        grad
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="grad">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.autodiff.variable.grad--todo-currently-serves-no-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        TODO currently serves no optimization
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.loss_NLL" class="md-nav__link">
    <span class="md-ellipsis">
      
        loss_NLL
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.loss_mae" class="md-nav__link">
    <span class="md-ellipsis">
      
        loss_mae
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.loss_mse" class="md-nav__link">
    <span class="md-ellipsis">
      
        loss_mse
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.operator_broadcast_to" class="md-nav__link">
    <span class="md-ellipsis">
      
        operator_broadcast_to
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.operator_flip" class="md-nav__link">
    <span class="md-ellipsis">
      
        operator_flip
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.operator_pad" class="md-nav__link">
    <span class="md-ellipsis">
      
        operator_pad
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.operator_reshape" class="md-nav__link">
    <span class="md-ellipsis">
      
        operator_reshape
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.variable.operator_sum" class="md-nav__link">
    <span class="md-ellipsis">
      
        operator_sum
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.tape" class="md-nav__link">
    <span class="md-ellipsis">
      
        tape
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.autodiff.tape.Tape" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tape
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.network.network" class="md-nav__link">
    <span class="md-ellipsis">
      
        network
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.network.network.Linear" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.network.network.Network" class="md-nav__link">
    <span class="md-ellipsis">
      
        Network
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>API Reference</h1>

<div class="doc doc-object doc-module">



<a id="src"></a>
    <div class="doc doc-contents first">

        <p>PyGrad - A neural network library with autograd automatic gradient calculation.</p>










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h2 id="src.Variable" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Variable</span>


<a href="#src.Variable" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">










              <details class="mkdocstrings-source">
                <summary>Source code in <code>src/autodiff/variable.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Variable</span><span class="p">:</span>
    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">numeric</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">numeric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="ow">or</span> <span class="n">free_name</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">)</span> <span class="k">else</span> <span class="n">array</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="c1"># TODO how to memoize everything by default?</span>
    <span class="nd">@staticmethod</span>
    <span class="nd">@lru_cache</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">constant</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">numeric</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;micro-optimization memoizing constant variables since they don&#39;t lead to cycles in the computation graph&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">convert_input_into_variable</span><span class="p">(</span>
        <span class="n">operator_impl</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]]:</span>
        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">operator_impl</span><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">operator_wrapped</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">raw_inputs</span><span class="p">:</span> <span class="n">numeric</span> <span class="o">|</span> <span class="n">Variable</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">operator_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">raw_inputs</span><span class="p">)</span>
            <span class="n">converted_args</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Variable</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">raw_other</span> <span class="ow">in</span> <span class="n">raw_inputs</span><span class="p">:</span>
                <span class="n">converted</span><span class="p">:</span> <span class="n">Variable</span>
                <span class="k">match</span> <span class="n">raw_other</span><span class="p">:</span>
                    <span class="k">case</span> <span class="n">ndarr</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ndarr</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">):</span>
                        <span class="c1"># TODO maybe check supported dimensions</span>
                        <span class="n">converted_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">raw_other</span><span class="p">))</span>
                    <span class="k">case</span> <span class="n">raw_numeric</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_numeric</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
                        <span class="n">converted_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Variable</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">raw_numeric</span><span class="p">))</span>
                    <span class="k">case</span> <span class="n">already_variable</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">already_variable</span><span class="p">,</span> <span class="n">Variable</span><span class="p">):</span>
                        <span class="n">converted_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">already_variable</span><span class="p">)</span>
                    <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Unsupported type for Variable operation: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">raw_other</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
            <span class="k">return</span> <span class="n">operator_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">converted_args</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">operator_wrapped</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">(value=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="c1"># operators have no explicit type checks for micro optimization as static typechecks should be enough</span>
    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">+</span> <span class="n">other</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_neg</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">-</span><span class="n">other</span><span class="p">)</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_add</span><span class="p">(</span><span class="o">-</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="c1"># if TYPE_SAFE:</span>
        <span class="c1">#     if self.shape != other.shape:</span>
        <span class="c1">#         other = self.broadcast_to(self.shape)</span>
        <span class="k">return</span> <span class="n">operator_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="c1"># elementwise multiplication forced commutative</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="n">other</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">ndarray</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">other</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;vectors must be explicitly padded with an extra dimension. Reshape as (..., n, 1)&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">other</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">reshape_vec_as_mat</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">operator_matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">ndarray</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_matmul</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reshape_vec_as_mat</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">flip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_flip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_width</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_width</span><span class="o">=</span><span class="n">pad_width</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">broadcast_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_broadcast_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">size</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">convolve</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">kernel</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convolve matrix with kernel, both&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;convolution arguments dimensionality must match.&quot;</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">all</span><span class="p">(</span><span class="n">matrix_shape</span> <span class="o">&gt;</span> <span class="n">kernel_shape</span>
                    <span class="k">for</span> <span class="n">matrix_shape</span><span class="p">,</span> <span class="n">kernel_shape</span> <span class="ow">in</span>
                    <span class="nb">zip</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
            <span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;Kernel size must be smaller than input size for convolution. Use elementwise multiplication instead.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">operator_convolution</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">ReLU</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">activation_ReLU</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">activation_sigmoid</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">activation_tanh</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))</span>
<span class="sd">        see the variable.activation_log_softmax() for more info</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">activation_log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">directional_grad</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convenience method to compute gradients of this variable with respect to all other variables in the graph.&quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">_tape_stack</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">var</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">custom_entry_gradient</span><span class="o">=</span><span class="n">directional_grad</span><span class="p">,</span> <span class="n">tape_records</span><span class="o">=</span><span class="n">_tape_stack</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="src.Variable.backward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">backward</span>


<a href="#src.Variable.backward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">backward</span><span class="p">(</span><span class="n">directional_grad</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Convenience method to compute gradients of this variable with respect to all other variables in the graph.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">directional_grad</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convenience method to compute gradients of this variable with respect to all other variables in the graph.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">var</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">custom_entry_gradient</span><span class="o">=</span><span class="n">directional_grad</span><span class="p">,</span> <span class="n">tape_records</span><span class="o">=</span><span class="n">_tape_stack</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="src.Variable.constant" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">constant</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-cached"><code>cached</code></small>
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#src.Variable.constant" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">constant</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">numeric</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="nd">@lru_cache</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">constant</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">numeric</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;micro-optimization memoizing constant variables since they don&#39;t lead to cycles in the computation graph&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="src.Variable.convolve" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">convolve</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#src.Variable.convolve" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">convolve</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">kernel</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Convolve matrix with kernel, both</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">convolve</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">kernel</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convolve matrix with kernel, both&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;convolution arguments dimensionality must match.&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">all</span><span class="p">(</span><span class="n">matrix_shape</span> <span class="o">&gt;</span> <span class="n">kernel_shape</span>
                <span class="k">for</span> <span class="n">matrix_shape</span><span class="p">,</span> <span class="n">kernel_shape</span> <span class="ow">in</span>
                <span class="nb">zip</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;Kernel size must be smaller than input size for convolution. Use elementwise multiplication instead.&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">operator_convolution</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="src.Variable.log_softmax" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">log_softmax</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#src.Variable.log_softmax" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Computes log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))
see the variable.activation_log_softmax() for more info</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))</span>
<span class="sd">    see the variable.activation_log_softmax() for more info</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">activation_log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="src.autodiff.variable"></a>
    <div class="doc doc-contents first">

        <p>@Author Adam Jamroziski</p>
<p>Variable is the building block of the computation graph
each overloaded or custom operator saves itself as a record on a tape stack</p>
<p>The _tape_stack is bundled with the variable module by default.
It contains all the information about which variable was needed to compute the other.
The information how to compute the backward partial derivative at each step.
Thanks to this approach, the list is being automatically sorted topologically (in reverse order)</p>
<p>Each variable in the computation graph can be passed to the grad(...) function to compute its derivative</p>










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h2 id="src.autodiff.variable.Variable" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Variable</span>


<a href="#src.autodiff.variable.Variable" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">










              <details class="mkdocstrings-source">
                <summary>Source code in <code>src/autodiff/variable.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Variable</span><span class="p">:</span>
    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">numeric</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">numeric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="ow">or</span> <span class="n">free_name</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">)</span> <span class="k">else</span> <span class="n">array</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="c1"># TODO how to memoize everything by default?</span>
    <span class="nd">@staticmethod</span>
    <span class="nd">@lru_cache</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">constant</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">numeric</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;micro-optimization memoizing constant variables since they don&#39;t lead to cycles in the computation graph&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">convert_input_into_variable</span><span class="p">(</span>
        <span class="n">operator_impl</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]]:</span>
        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">operator_impl</span><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">operator_wrapped</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">raw_inputs</span><span class="p">:</span> <span class="n">numeric</span> <span class="o">|</span> <span class="n">Variable</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">operator_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">raw_inputs</span><span class="p">)</span>
            <span class="n">converted_args</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Variable</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">raw_other</span> <span class="ow">in</span> <span class="n">raw_inputs</span><span class="p">:</span>
                <span class="n">converted</span><span class="p">:</span> <span class="n">Variable</span>
                <span class="k">match</span> <span class="n">raw_other</span><span class="p">:</span>
                    <span class="k">case</span> <span class="n">ndarr</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ndarr</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">):</span>
                        <span class="c1"># TODO maybe check supported dimensions</span>
                        <span class="n">converted_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">raw_other</span><span class="p">))</span>
                    <span class="k">case</span> <span class="n">raw_numeric</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_numeric</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
                        <span class="n">converted_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Variable</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">raw_numeric</span><span class="p">))</span>
                    <span class="k">case</span> <span class="n">already_variable</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">already_variable</span><span class="p">,</span> <span class="n">Variable</span><span class="p">):</span>
                        <span class="n">converted_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">already_variable</span><span class="p">)</span>
                    <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Unsupported type for Variable operation: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">raw_other</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
            <span class="k">return</span> <span class="n">operator_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">converted_args</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">operator_wrapped</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">(value=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="c1"># operators have no explicit type checks for micro optimization as static typechecks should be enough</span>
    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">+</span> <span class="n">other</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_neg</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">-</span><span class="n">other</span><span class="p">)</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_add</span><span class="p">(</span><span class="o">-</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="c1"># if TYPE_SAFE:</span>
        <span class="c1">#     if self.shape != other.shape:</span>
        <span class="c1">#         other = self.broadcast_to(self.shape)</span>
        <span class="k">return</span> <span class="n">operator_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span><span class="p">):</span>
        <span class="c1"># elementwise multiplication forced commutative</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="n">other</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">ndarray</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">other</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;vectors must be explicitly padded with an extra dimension. Reshape as (..., n, 1)&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">other</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">reshape_vec_as_mat</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">operator_matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@convert_input_into_variable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">ndarray</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_matmul</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reshape_vec_as_mat</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">flip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_flip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_width</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_width</span><span class="o">=</span><span class="n">pad_width</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">operator_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">broadcast_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_broadcast_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">operator_transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">size</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">convolve</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">kernel</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convolve matrix with kernel, both&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;convolution arguments dimensionality must match.&quot;</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">all</span><span class="p">(</span><span class="n">matrix_shape</span> <span class="o">&gt;</span> <span class="n">kernel_shape</span>
                    <span class="k">for</span> <span class="n">matrix_shape</span><span class="p">,</span> <span class="n">kernel_shape</span> <span class="ow">in</span>
                    <span class="nb">zip</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
            <span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;Kernel size must be smaller than input size for convolution. Use elementwise multiplication instead.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">operator_convolution</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">ReLU</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">activation_ReLU</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">activation_sigmoid</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">activation_tanh</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))</span>
<span class="sd">        see the variable.activation_log_softmax() for more info</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">activation_log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">directional_grad</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convenience method to compute gradients of this variable with respect to all other variables in the graph.&quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">_tape_stack</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">var</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">custom_entry_gradient</span><span class="o">=</span><span class="n">directional_grad</span><span class="p">,</span> <span class="n">tape_records</span><span class="o">=</span><span class="n">_tape_stack</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="src.autodiff.variable.Variable.backward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">backward</span>


<a href="#src.autodiff.variable.Variable.backward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">backward</span><span class="p">(</span><span class="n">directional_grad</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Convenience method to compute gradients of this variable with respect to all other variables in the graph.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">directional_grad</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convenience method to compute gradients of this variable with respect to all other variables in the graph.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">var</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">custom_entry_gradient</span><span class="o">=</span><span class="n">directional_grad</span><span class="p">,</span> <span class="n">tape_records</span><span class="o">=</span><span class="n">_tape_stack</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="src.autodiff.variable.Variable.constant" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">constant</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-cached"><code>cached</code></small>
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#src.autodiff.variable.Variable.constant" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">constant</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">numeric</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>micro-optimization memoizing constant variables since they don't lead to cycles in the computation graph</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="nd">@lru_cache</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">constant</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">numeric</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;micro-optimization memoizing constant variables since they don&#39;t lead to cycles in the computation graph&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="src.autodiff.variable.Variable.convolve" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">convolve</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#src.autodiff.variable.Variable.convolve" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">convolve</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">kernel</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Convolve matrix with kernel, both</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">convolve</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">kernel</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convolve matrix with kernel, both&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;convolution arguments dimensionality must match.&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">all</span><span class="p">(</span><span class="n">matrix_shape</span> <span class="o">&gt;</span> <span class="n">kernel_shape</span>
                <span class="k">for</span> <span class="n">matrix_shape</span><span class="p">,</span> <span class="n">kernel_shape</span> <span class="ow">in</span>
                <span class="nb">zip</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;Kernel size must be smaller than input size for convolution. Use elementwise multiplication instead.&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">operator_convolution</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="src.autodiff.variable.Variable.log_softmax" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">log_softmax</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#src.autodiff.variable.Variable.log_softmax" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Computes log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))
see the variable.activation_log_softmax() for more info</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))</span>
<span class="sd">    see the variable.activation_log_softmax() for more info</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">activation_log_softmax</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable._ONES" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">_ONES</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-cached"><code>cached</code></small>
  </span>

<a href="#src.autodiff.variable._ONES" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">_ONES</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Returns cached Variable of ones with a given shape and type</p>
<p>Thus this should never be a learnable parameter (must stay immutable)</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_ONES</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns cached Variable of ones with a given shape and type</span>

<span class="sd">    Thus this should never be a learnable parameter (must stay immutable)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">ones</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable._ZEROS" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">_ZEROS</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-cached"><code>cached</code></small>
  </span>

<a href="#src.autodiff.variable._ZEROS" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">_ZEROS</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Returns cached Variable of zeros with a given shape and type</p>
<p>Thus this should never be a learnable parameter (must stay immutable)</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_ZEROS</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns cached Variable of zeros with a given shape and type</span>

<span class="sd">    Thus this should never be a learnable parameter (must stay immutable)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">zeros</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable._image2windows" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">_image2windows</span>


<a href="#src.autodiff.variable._image2windows" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">_image2windows</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">window_shape</span><span class="p">:</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ndarray</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Extracts a view of sliding windows from the input image based on windws_shape.</p>
<p>Assumes no padding and up/down sliding stride of 1.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_image2windows</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">window_shape</span><span class="p">:</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts a view of sliding windows from the input image based on windws_shape.</span>

<span class="sd">    Assumes no padding and up/down sliding stride of 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">i_rows</span><span class="p">,</span> <span class="n">i_cols</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">w_rows</span><span class="p">,</span> <span class="n">w_cols</span> <span class="o">=</span> <span class="n">window_shape</span>
    <span class="n">output_rows</span> <span class="o">=</span> <span class="n">i_rows</span> <span class="o">-</span> <span class="n">w_rows</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">output_cols</span> <span class="o">=</span> <span class="n">i_cols</span> <span class="o">-</span> <span class="n">w_cols</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="n">sliding_windows_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_rows</span><span class="p">,</span> <span class="n">output_cols</span><span class="p">,</span> <span class="n">w_rows</span><span class="p">,</span> <span class="n">w_cols</span><span class="p">)</span>
    <span class="n">sliding_windows_strides</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">image</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">image</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">image</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">image</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">as_strided</span><span class="p">(</span>
        <span class="n">image</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">sliding_windows_shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">sliding_windows_strides</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable._space2windows" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">_space2windows</span>


<a href="#src.autodiff.variable._space2windows" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">_space2windows</span><span class="p">(</span><span class="n">space</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">window_shape</span><span class="p">:</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ndarray</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Extracts a view of sliding windows from the input space based on windws_shape.
Assumes no padding and sliding stride of 1 in all dimensions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_space2windows</span><span class="p">(</span><span class="n">space</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">window_shape</span><span class="p">:</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts a view of sliding windows from the input space based on windws_shape.</span>
<span class="sd">    Assumes no padding and sliding stride of 1 in all dimensions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s_1d</span><span class="p">,</span> <span class="n">s_2d</span><span class="p">,</span> <span class="n">s_3d</span> <span class="o">=</span> <span class="n">space</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">w_1d</span><span class="p">,</span> <span class="n">w_2d</span><span class="p">,</span> <span class="n">w_3d</span> <span class="o">=</span> <span class="n">window_shape</span>
    <span class="n">output_1d</span> <span class="o">=</span> <span class="n">s_1d</span> <span class="o">-</span> <span class="n">w_1d</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">output_2d</span> <span class="o">=</span> <span class="n">s_2d</span> <span class="o">-</span> <span class="n">w_2d</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">output_3d</span> <span class="o">=</span> <span class="n">s_3d</span> <span class="o">-</span> <span class="n">w_3d</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="n">sliding_windows_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_1d</span><span class="p">,</span> <span class="n">output_2d</span><span class="p">,</span> <span class="n">output_3d</span><span class="p">,</span> <span class="n">w_1d</span><span class="p">,</span> <span class="n">w_2d</span><span class="p">,</span> <span class="n">w_3d</span><span class="p">)</span>
    <span class="n">sliding_windows_strides</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">space</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">space</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">space</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">space</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">space</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">space</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">as_strided</span><span class="p">(</span>
        <span class="n">space</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">sliding_windows_shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">sliding_windows_strides</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable._stream2windows" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">_stream2windows</span>


<a href="#src.autodiff.variable._stream2windows" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">_stream2windows</span><span class="p">(</span><span class="n">stream</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">window_length</span><span class="p">:</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ndarray</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Extracts a view of sliding windows from the input 1D stream based on window_length.
Assumes no padding and left/right sliding stride of 1.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_stream2windows</span><span class="p">(</span><span class="n">stream</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">window_length</span><span class="p">:</span> <span class="n">shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts a view of sliding windows from the input 1D stream based on window_length.</span>
<span class="sd">    Assumes no padding and left/right sliding stride of 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="p">(</span><span class="n">stream_length</span><span class="p">,)</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">shape</span>
    <span class="p">(</span><span class="n">window_length</span><span class="p">,)</span> <span class="o">=</span> <span class="n">window_length</span>
    <span class="n">output_length</span> <span class="o">=</span> <span class="n">stream_length</span> <span class="o">-</span> <span class="n">window_length</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="n">sliding_windows_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_length</span><span class="p">,</span> <span class="n">window_length</span><span class="p">)</span>
    <span class="n">sliding_windows_strides</span> <span class="o">=</span> <span class="p">(</span><span class="n">stream</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stream</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">as_strided</span><span class="p">(</span>
        <span class="n">stream</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">sliding_windows_shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">sliding_windows_strides</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.activation_ReLU" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">activation_ReLU</span>


<a href="#src.autodiff.variable.activation_ReLU" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">activation_ReLU</span><span class="p">(</span><span class="n">var</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Element-wise Rectified Linear Unit activation function</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">activation_ReLU</span><span class="p">(</span><span class="n">var</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Element-wise Rectified Linear Unit activation function&quot;&quot;&quot;</span>

    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="n">dLoss_dInput</span> <span class="o">=</span> <span class="n">dLoss_dResult</span> <span class="o">*</span> <span class="n">Variable</span><span class="p">((</span><span class="n">var</span><span class="o">.</span><span class="n">value</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">))</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dInput</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.activation_log_softmax" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">activation_log_softmax</span>


<a href="#src.autodiff.variable.activation_log_softmax" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">activation_log_softmax</span><span class="p">(</span><span class="n">var</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Element-wise Log(Softmax(var)) activation function along the specified axis</p>
<p>Use it with Negative-Log-Likelihood to achieve cross-entropy loss.</p>
<p>Note that for regular softmax the derivatives would have looked like
    dSoftmax_i/dInput_i = Softmax_i * (1 - Softmax_i)
    dSoftmax_i/dInput_j = -Softmax_i * Softmax_j; if i != j
however with log it simplifies to:
    dLogSoftmax_i/dInput_i = 1 - Softmax_i
    dLogSoftmax_i/dInput_j = -Softmax_j; if i != j</p>
<p>This implementation includes the standard numerical stability trick with the max subtraction.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">activation_log_softmax</span><span class="p">(</span><span class="n">var</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Log(Softmax(var)) activation function along the specified axis</span>

<span class="sd">    Use it with Negative-Log-Likelihood to achieve cross-entropy loss.</span>

<span class="sd">    Note that for regular softmax the derivatives would have looked like</span>
<span class="sd">        dSoftmax_i/dInput_i = Softmax_i * (1 - Softmax_i)</span>
<span class="sd">        dSoftmax_i/dInput_j = -Softmax_i * Softmax_j; if i != j</span>
<span class="sd">    however with log it simplifies to:</span>
<span class="sd">        dLogSoftmax_i/dInput_i = 1 - Softmax_i</span>
<span class="sd">        dLogSoftmax_i/dInput_j = -Softmax_j; if i != j</span>

<span class="sd">    This implementation includes the standard numerical stability trick with the max subtraction.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">var_value</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">value</span>
    <span class="n">normalized_exp_values</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">var_value</span> <span class="o">-</span> <span class="n">var_value</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="c1"># log-sum-exp stability trick</span>
    <span class="n">log_softmax_values</span> <span class="o">=</span> <span class="n">normalized_exp_values</span> <span class="o">-</span> <span class="n">log</span><span class="p">(</span>
        <span class="n">normalized_exp_values</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">log_softmax_values</span><span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="c1"># Jacobian matrix for log_softmax is complex but gives more control over the learning process</span>
        <span class="c1"># dLoss_dInput = (</span>
        <span class="c1">#     dLoss_dResult.broadcast_to((*var.shape, *dLoss_dResult.shape)) @ -forward</span>
        <span class="c1">#     + dLoss_dResult</span>
        <span class="c1"># )</span>
        <span class="n">softmax</span> <span class="o">=</span> <span class="n">forward</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
        <span class="n">sum_over_axis</span> <span class="o">=</span> <span class="n">dLoss_dResult</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">dLoss_dInput</span> <span class="o">=</span> <span class="n">dLoss_dResult</span> <span class="o">-</span> <span class="p">(</span><span class="n">softmax</span> <span class="o">*</span> <span class="n">sum_over_axis</span><span class="p">)</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dInput</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.activation_sigmoid" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">activation_sigmoid</span>


<a href="#src.autodiff.variable.activation_sigmoid" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">activation_sigmoid</span><span class="p">(</span><span class="n">var</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Element-wise Sigmoid activation function</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">activation_sigmoid</span><span class="p">(</span><span class="n">var</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Element-wise Sigmoid activation function&quot;&quot;&quot;</span>

    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">var</span><span class="o">.</span><span class="n">value</span><span class="p">)))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="n">dLoss_dInput</span> <span class="o">=</span> <span class="n">dLoss_dResult</span> <span class="o">*</span> <span class="p">(</span><span class="n">forward</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">forward</span><span class="p">))</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dInput</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.activation_tanh" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">activation_tanh</span>


<a href="#src.autodiff.variable.activation_tanh" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">activation_tanh</span><span class="p">(</span><span class="n">var</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Element-wise Tanh activation function</p>
<p>tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">activation_tanh</span><span class="p">(</span><span class="n">var</span><span class="p">:</span> <span class="n">Variable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Tanh activation function</span>

<span class="sd">    tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tanh</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="n">dLoss_dInput</span> <span class="o">=</span> <span class="n">dLoss_dResult</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">forward</span> <span class="o">*</span> <span class="n">forward</span><span class="p">)</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dInput</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.convolve_forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">convolve_forward</span>


<a href="#src.autodiff.variable.convolve_forward" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">convolve_forward</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">kernel</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">optimize</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;greedy&#39;</span><span class="p">,</span> <span class="s1">&#39;optimal&#39;</span><span class="p">]</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">|</span> <span class="n">Sequence</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ndarray</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Performs a convolution operation on the input ndarray with the given kernel.</p>
<p>Matrix and Kernel must adhere to constraints.
This library version supports 1d,2d and 3d convolutions, but more dimensions are straightforward to implement.
Note that if the length of dimensions of tensor and kernel match, custom 4D, 5D, and so on sliding windows
result in the exact same backward functions!</p>
<p>This implementation does not flip the kernel, effectively performing a Convolution* a.k.a. Cross-Correlation.
Note we do not perform padding and assume a stride of 1.</p>
<p>The 'optimize' param is passed to the numpy.einsum for optimized broadcasted multiplication and reduction sum
with no optimization the broadcasting with einsum is already faster because it doesn't create copies of intermediate arrays.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">convolve_forward</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span>
    <span class="n">optimize</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;greedy&quot;</span><span class="p">,</span> <span class="s2">&quot;optimal&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">|</span> <span class="n">Sequence</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a convolution operation on the input ndarray with the given kernel.</span>

<span class="sd">    Matrix and Kernel must adhere to constraints.</span>
<span class="sd">    This library version supports 1d,2d and 3d convolutions, but more dimensions are straightforward to implement.</span>
<span class="sd">    Note that if the length of dimensions of tensor and kernel match, custom 4D, 5D, and so on sliding windows</span>
<span class="sd">    result in the exact same backward functions!</span>

<span class="sd">    This implementation does not flip the kernel, effectively performing a Convolution* a.k.a. Cross-Correlation.</span>
<span class="sd">    Note we do not perform padding and assume a stride of 1.</span>

<span class="sd">    The &#39;optimize&#39; param is passed to the numpy.einsum for optimized broadcasted multiplication and reduction sum</span>
<span class="sd">    with no optimization the broadcasting with einsum is already faster because it doesn&#39;t create copies of intermediate arrays.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">match</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">case</span><span class="w"> </span><span class="p">(</span><span class="k">_</span><span class="p">,):</span>
            <span class="n">windows</span> <span class="o">=</span> <span class="n">_stream2windows</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">subscript</span> <span class="o">=</span> <span class="s2">&quot;ij,j-&gt;i&quot;</span>
        <span class="k">case</span><span class="w"> </span><span class="p">(</span><span class="k">_</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
            <span class="n">windows</span> <span class="o">=</span> <span class="n">_image2windows</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">subscript</span> <span class="o">=</span> <span class="s2">&quot;ijkl,kl-&gt;ij&quot;</span>
        <span class="k">case</span><span class="w"> </span><span class="p">(</span><span class="k">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
            <span class="n">windows</span> <span class="o">=</span> <span class="n">_space2windows</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">subscript</span> <span class="o">=</span> <span class="s2">&quot;ijklmn,lmn-&gt;ijk&quot;</span>
        <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This type of convolution needs a custom implementation.&quot;</span><span class="p">)</span>

    <span class="c1"># broadcast multiplication and sum over the last kernel.ndim axis</span>
    <span class="k">return</span> <span class="n">einsum</span><span class="p">(</span><span class="n">subscript</span><span class="p">,</span> <span class="n">windows</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.get_tape_stack_snapshot" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_tape_stack_snapshot</span>


<a href="#src.autodiff.variable.get_tape_stack_snapshot" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">get_tape_stack_snapshot</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tape</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>returns a snapshot shallow copy of the tape stack</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_tape_stack_snapshot</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tape</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;returns a snapshot shallow copy of the tape stack&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="k">return</span> <span class="n">_tape_stack</span><span class="p">[:]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.grad" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">grad</span>


<a href="#src.autodiff.variable.grad" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">grad</span><span class="p">(</span><span class="n">loss_variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">tape_records</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_entry_gradient</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">desired_results</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">Variable</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Computes gradients of the loss_variable with respect to each Variable in the computation graph,
dLoss_d lookup map effectively works out to make dLoss_d[x] equal to dLoss/dx</p>
<p>:returns
    a dictionary of the d(loss_variable)/d[key] where key is any other variable used to compute the loss_variable</p>
<p>:params
loss_variable:
    The top node of the computation graph representing the loss.
    If the loss is not a scalar, it's implicitly set to be an array of ones with the same shape as the loss.</p>


<details class="desired_results" open>
  <summary>desired_results</summary>
  <h3 id="src.autodiff.variable.grad--todo-currently-serves-no-optimization">TODO currently serves no optimization<a class="headerlink" href="#src.autodiff.variable.grad--todo-currently-serves-no-optimization" title="Permanent link">&para;</a></h3>
<p>Selects what we exclude from the gradient result list.
If desired_results is None, gradients for all variables in the computation graph will be computed.</p>
</details>        <p>If the tape does not contain a variable,
we consider its gradient None (which brings pruning of unused graoh branches to constant time checks).</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">grad</span><span class="p">(</span>
    <span class="n">loss_variable</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">tape_records</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_entry_gradient</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">desired_results</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">Variable</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients of the loss_variable with respect to each Variable in the computation graph,</span>
<span class="sd">    dLoss_d lookup map effectively works out to make dLoss_d[x] equal to dLoss/dx</span>

<span class="sd">    :returns</span>
<span class="sd">        a dictionary of the d(loss_variable)/d[key] where key is any other variable used to compute the loss_variable</span>

<span class="sd">    :params</span>
<span class="sd">    loss_variable:</span>
<span class="sd">        The top node of the computation graph representing the loss.</span>
<span class="sd">        If the loss is not a scalar, it&#39;s implicitly set to be an array of ones with the same shape as the loss.</span>

<span class="sd">    desired_results:</span>
<span class="sd">        # TODO currently serves no optimization</span>
<span class="sd">        Selects what we exclude from the gradient result list.</span>
<span class="sd">        If desired_results is None, gradients for all variables in the computation graph will be computed.</span>

<span class="sd">    If the tape does not contain a variable,</span>
<span class="sd">    we consider its gradient None (which brings pruning of unused graoh branches to constant time checks).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">tape_records</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tape_records</span> <span class="o">=</span> <span class="n">get_tape_stack_snapshot</span><span class="p">()</span>
    <span class="n">dLoss_d</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="n">Variable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">loss_variable</span><span class="p">:</span> <span class="n">_ONES</span><span class="p">(</span><span class="n">loss_variable</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">loss_variable</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">custom_entry_gradient</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">custom_entry_gradient</span>
    <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prune_unused_outputs</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="p">(</span><span class="n">dLoss_d</span><span class="p">[</span><span class="n">output</span><span class="p">]</span> <span class="k">if</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">dLoss_d</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span>
        <span class="p">)</span>

    <span class="k">for</span> <span class="n">tape_record</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">tape_records</span><span class="p">):</span>
        <span class="n">dLoss_dOutputs</span> <span class="o">=</span> <span class="n">prune_unused_outputs</span><span class="p">(</span><span class="n">tape_record</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">dL_dOutput</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">dL_dOutput</span> <span class="ow">in</span> <span class="n">dLoss_dOutputs</span><span class="p">):</span>
            <span class="k">continue</span>  <span class="c1"># prune paths equal to zero vectors</span>

        <span class="c1"># perform chain rule back propagation</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="n">tape_record</span><span class="o">.</span><span class="n">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">)</span>

        <span class="c1"># due to unconstrained computation graph shape we actually sum all the gradients</span>
        <span class="c1"># the automatic topological sorting of the tape ensures we use the final dLoss_d[tape_input] value</span>
        <span class="c1"># only when it was fully computed</span>
        <span class="k">for</span> <span class="n">tape_input</span><span class="p">,</span> <span class="n">dL_dInput</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tape_record</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dLoss_dInputs</span><span class="p">):</span>
            <span class="c1"># we could have used default(lambda x:zeros) but this way we keep the notion of what was used in the process</span>
            <span class="k">if</span> <span class="n">tape_input</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dLoss_d</span><span class="p">:</span>
                <span class="n">dLoss_d</span><span class="p">[</span><span class="n">tape_input</span><span class="p">]</span> <span class="o">=</span> <span class="n">dL_dInput</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dLoss_d</span><span class="p">[</span><span class="n">tape_input</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dL_dInput</span>

    <span class="c1"># debug information values of each intermediate gradient</span>
    <span class="c1"># for name, value in dLoss_d.items():</span>
    <span class="c1">#     print(f&quot;d{loss_variable.name}_d{name} = {value.name}&quot;)</span>

    <span class="k">return</span> <span class="n">dLoss_d</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.loss_NLL" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">loss_NLL</span>


<a href="#src.autodiff.variable.loss_NLL" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">loss_NLL</span><span class="p">(</span><span class="n">log_probs</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">target_probs</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Negative Log-Likelihood Loss for classification tasks.</p>
<p>:param log_probs: Log probabilities from the model (output of log_softmax).
:param target_probs: True underlying probability indices (class labels).
:return: NLL loss variable. Combined with log_softmax gives cross-entropy loss.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@Variable</span><span class="o">.</span><span class="n">convert_input_into_variable</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss_NLL</span><span class="p">(</span><span class="n">log_probs</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">target_probs</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Negative Log-Likelihood Loss for classification tasks.</span>

<span class="sd">    :param log_probs: Log probabilities from the model (output of log_softmax).</span>
<span class="sd">    :param target_probs: True underlying probability indices (class labels).</span>
<span class="sd">    :return: NLL loss variable. Combined with log_softmax gives cross-entropy loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nll_values</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_probs</span> <span class="o">*</span> <span class="n">target_probs</span>
    <span class="n">nll_loss</span> <span class="o">=</span> <span class="n">nll_values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nll_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.loss_mae" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">loss_mae</span>


<a href="#src.autodiff.variable.loss_mae" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">loss_mae</span><span class="p">(</span><span class="n">predicted</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">numeric</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Mean Absolute Error Loss between predicted and target variables.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@Variable</span><span class="o">.</span><span class="n">convert_input_into_variable</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss_mae</span><span class="p">(</span>
    <span class="n">predicted</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">numeric</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mean Absolute Error Loss between predicted and target variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">err</span> <span class="o">=</span> <span class="n">subtract</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="n">err_variable</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="n">mae_value</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">err</span><span class="p">)))</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">mae_value</span><span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted</span><span class="p">,)</span>  <span class="c1"># the target is a constant, so skip it here</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">mae</span><span class="p">,)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">):</span>
        <span class="p">(</span><span class="n">dLoss_dOutput</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="c1"># note that mae may bug the higher order gradients</span>
        <span class="n">dLoss_dInput</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">err</span><span class="p">))</span> <span class="o">*</span> <span class="n">dLoss_dOutput</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dLoss_dInput</span><span class="p">,)</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">mae</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.loss_mse" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">loss_mse</span>


<a href="#src.autodiff.variable.loss_mse" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">loss_mse</span><span class="p">(</span><span class="n">predicted</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">numeric</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Mean Squared Error Loss between predicted and target variables.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@Variable</span><span class="o">.</span><span class="n">convert_input_into_variable</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss_mse</span><span class="p">(</span>
    <span class="n">predicted</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Variable</span> <span class="o">|</span> <span class="n">numeric</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">numeric</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mean Squared Error Loss between predicted and target variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">err</span> <span class="o">=</span> <span class="n">subtract</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="c1"># swap for simplicity</span>
    <span class="n">err_variable</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="n">mse_value</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">square</span><span class="p">(</span><span class="n">err</span><span class="p">)))</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">mse_value</span><span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted</span><span class="p">,)</span>  <span class="c1"># the target is a constant, so skip it here</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">mse</span><span class="p">,)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">):</span>
        <span class="p">(</span><span class="n">dLoss_dOutput</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="c1"># ignore the factor of 2</span>
        <span class="n">dLoss_dInput</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dOutput</span> <span class="o">*</span> <span class="n">err_variable</span><span class="p">)</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dLoss_dInput</span><span class="p">,)</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">mse</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.operator_broadcast_to" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">operator_broadcast_to</span>


<a href="#src.autodiff.variable.operator_broadcast_to" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">operator_broadcast_to</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Broadcasts the array variable to the given shape
equivalent to numpy.broadcast_to</p>
<p>Beware this function uses numpy's view under the hood
so the result of this operator must remain immutable.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">operator_broadcast_to</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts the array variable to the given shape</span>
<span class="sd">    equivalent to numpy.broadcast_to</span>

<span class="sd">    Beware this function uses numpy&#39;s view under the hood</span>
<span class="sd">    so the result of this operator must remain immutable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">broadcast_shape</span><span class="p">))</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dBroadcastedResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>

        <span class="c1"># sum over the broadcasted dimensions</span>
        <span class="n">reduce_extra_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="n">dLoss_dBroadcastedResult</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">tensor</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">intermediate_sum</span> <span class="o">=</span> <span class="n">dLoss_dBroadcastedResult</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">reduce_extra_axes</span><span class="p">)</span>
        <span class="n">reduce_inner_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">axis</span>
            <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">intermediate_sum</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">dim1</span> <span class="o">!=</span> <span class="n">dim2</span>
        <span class="p">)</span>
        <span class="n">dLoss_dTensor</span> <span class="o">=</span> <span class="n">dLoss_dBroadcastedResult</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="n">axis</span><span class="o">=</span><span class="n">reduce_inner_axes</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">TYPE_SAFE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">dLoss_dTensor</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;shape mismatch in broadcast_to backward pass&quot;</span>
            <span class="p">)</span>

        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dTensor</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.operator_flip" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">operator_flip</span>


<a href="#src.autodiff.variable.operator_flip" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">operator_flip</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>flip the array variable along dimension
equivalent to numpy.flip</p>
<p>operator_flip(array) is equivalent to array[::-1,::-1, ..., ::-1]</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">operator_flip</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    flip the array variable along dimension</span>
<span class="sd">    equivalent to numpy.flip</span>

<span class="sd">    operator_flip(array) is equivalent to array[::-1,::-1, ..., ::-1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">flip</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">matrix</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dRotatedResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="n">dLoss_dMatrix</span> <span class="o">=</span> <span class="n">dLoss_dRotatedResult</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dMatrix</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.operator_pad" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">operator_pad</span>


<a href="#src.autodiff.variable.operator_pad" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">operator_pad</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">pad_width</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Pads the array variable according to pad_width
pad_width ~ ((before(dim), after(dim)) for dim in each dimension)</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">operator_pad</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">pad_width</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the array variable according to pad_width</span>
<span class="sd">    pad_width ~ ((before(dim), after(dim)) for dim in each dimension)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">pad</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">pad_width</span><span class="o">=</span><span class="n">pad_width</span><span class="p">))</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">matrix</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dPaddedResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>

        <span class="c1"># slices = tuple(</span>
        <span class="c1">#     slice(pad_width_dim[0], dLoss_dPaddedResult.value.shape[i] - pad_width_dim[1])</span>
        <span class="c1">#     for i, pad_width_dim in enumerate(pad_width)</span>
        <span class="c1"># )</span>
        <span class="c1"># dLoss_dMatrix = Variable(dLoss_dPaddedResult.value[slices])</span>

        <span class="n">dLoss_dMatrix</span> <span class="o">=</span> <span class="n">matrix</span>  <span class="c1"># variables are immutable during grad computation, so only link dL_dIn matrix with dLdOut matrix padded with constants</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dMatrix</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.operator_reshape" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">operator_reshape</span>


<a href="#src.autodiff.variable.operator_reshape" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">operator_reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Reshape tensor using numpy.reshape</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">operator_reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshape tensor using numpy.reshape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="n">dLoss_dInput</span> <span class="o">=</span> <span class="n">dLoss_dResult</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dInput</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="src.autodiff.variable.operator_sum" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">operator_sum</span>


<a href="#src.autodiff.variable.operator_sum" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature codehilite"><pre><span></span><code><span class="nf">operator_sum</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Sums the elements of the tensor along the specified axis.
If axis is None, sums all elements.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>src/autodiff/variable.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">operator_sum</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sums the elements of the tensor along the specified axis.</span>
<span class="sd">    If axis is None, sums all elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">forward</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">))</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">forward</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">back_fn</span><span class="p">(</span><span class="n">dLoss_dOutputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">dLoss_dSumResult</span><span class="p">,)</span> <span class="o">=</span> <span class="n">dLoss_dOutputs</span>
        <span class="c1"># without expand_dims this may crash, but it is no bug, this version disallows such sum/broadcasting</span>
        <span class="n">dLoss_dTensor</span> <span class="o">=</span> <span class="n">dLoss_dSumResult</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">dLoss_dInputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dLoss_dTensor</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">dLoss_dInputs</span>

    <span class="k">global</span> <span class="n">_tape_stack</span>
    <span class="n">_tape_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tape</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">back_fn</span><span class="o">=</span><span class="n">back_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">forward</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="src.autodiff.tape"></a>
    <div class="doc doc-contents first">

        <p>The backward pass is computed via a topologically sorted stack of callbacks aka the backward pass is "tape-based".
Tape class organizes and singles out Variable's responsibility of storing the computation graph (DAG).
Additionally, it is a neat way we topologically sort the computation graph,
since the computation is going to be performed in the same order we put it on the tape.</p>










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h2 id="src.autodiff.tape.Tape" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Tape</span>


<a href="#src.autodiff.tape.Tape" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="typing.NamedTuple">NamedTuple</span></code></p>



        <p>Single tape record containing information about a single computation step in the graph</p>
<p>Remark that the back_fn callback operates on Variable instances
that entails that a new graph is being built during the backward pass</p>








              <details class="mkdocstrings-source">
                <summary>Source code in <code>src/autodiff/tape.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Tape</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Single tape record containing information about a single computation step in the graph</span>

<span class="sd">    Remark that the back_fn callback operates on Variable instances</span>
<span class="sd">    that entails that a new graph is being built during the backward pass</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">outputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]</span>
    <span class="c1"># back_fn: BackFunction</span>
    <span class="n">back_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]]</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="src.network.network"></a>
    <div class="doc doc-contents first">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h2 id="src.network.network.Linear" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Linear</span>


<a href="#src.network.network.Linear" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="Network (src.network.network.Network)" href="#src.network.network.Network">Network</a></code></p>



        <p>Ready to use linear subnetwork implementation
equivalent to Wx+b linear step</p>








              <details class="mkdocstrings-source">
                <summary>Source code in <code>src/network/network.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span>
<span class="normal">99</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Linear</span><span class="p">(</span><span class="n">Network</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Ready to use linear subnetwork implementation</span>
<span class="sd">    equivalent to Wx+b linear step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">random</span><span class="p">((</span><span class="n">out_features_length</span><span class="p">,</span> <span class="n">in_features_length</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span>
            <span class="n">random</span><span class="p">(</span>
                <span class="p">(</span><span class="n">out_features_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="c1"># todo reduce 2 operations into one by special operator_lin_step(w, b, x)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="src.network.network.Network" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Network</span>


<a href="#src.network.network.Network" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="abc.ABC">ABC</span></code></p>



        <p>Modular neural network base class.</p>
<p>To use, inherit from Network e.g.:
FooNN(Network):
    <strong>init</strong>(self, ...):
        super().<strong>init</strong>()
        declared here fields either of type Variable or Network
        will be stored marked as learnable parameters
        ...
    forward(self, ...):
        ...
        overwrite of abstract forward is being required
        this function needs to return the output Variable</p>








              <details class="mkdocstrings-source">
                <summary>Source code in <code>src/network/network.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Network</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Modular neural network base class.</span>

<span class="sd">    To use, inherit from Network e.g.:</span>
<span class="sd">    FooNN(Network):</span>
<span class="sd">        __init__(self, ...):</span>
<span class="sd">            super().__init__()</span>
<span class="sd">            declared here fields either of type Variable or Network</span>
<span class="sd">            will be stored marked as learnable parameters</span>
<span class="sd">            ...</span>
<span class="sd">        forward(self, ...):</span>
<span class="sd">            ...</span>
<span class="sd">            overwrite of abstract forward is being required</span>
<span class="sd">            this function needs to return the output Variable</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;__dict__&quot;</span><span class="p">,</span> <span class="s2">&quot;_parameters&quot;</span><span class="p">,</span> <span class="s2">&quot;_subnetworks&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_parameters&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_subnetworks&quot;</span><span class="p">,</span> <span class="p">[])</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_parameters&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subnetworks</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_subnetworks&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">parameters</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">subnetworks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot assign attributes to Network before </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.__init__() initialization.&quot;</span>
            <span class="p">)</span>

        <span class="k">match</span> <span class="n">value</span><span class="p">:</span>
            <span class="k">case</span> <span class="n">Variable</span><span class="p">():</span>
                <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">subnetworks</span><span class="p">:</span>
                    <span class="k">del</span> <span class="n">subnetworks</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">case</span> <span class="n">Network</span><span class="p">():</span>
                <span class="n">subnetworks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
                    <span class="k">del</span> <span class="n">parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

        <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]</span> <span class="o">|</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">Variable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Variable</span><span class="p">]</span> <span class="o">|</span> <span class="n">Variable</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Variable</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">chain</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">param</span>
                <span class="k">for</span> <span class="n">subnetwork</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subnetworks</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">subnetwork</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">subnetworks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="s1">&#39;Network&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">chain</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_subnetworks</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">sub_subnetwork</span>
                <span class="k">for</span> <span class="n">subnetwork</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subnetworks</span>
                <span class="k">for</span> <span class="n">sub_subnetwork</span> <span class="ow">in</span> <span class="n">subnetwork</span><span class="o">.</span><span class="n">subnetworks</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/adamJamro/pygrad" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["navigation.instant", "navigation.tabs", "navigation.sections", "content.code.copy", "toc.integrate"], "search": "../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
    
  </body>
</html>